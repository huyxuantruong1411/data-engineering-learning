=== Project Tree ===
D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites/
â”œâ”€â”€ clear_collections.py
â”œâ”€â”€ debug_animeplanet_enhanced.py
â”œâ”€â”€ dump.py
â”œâ”€â”€ run_conservative.py
â”œâ”€â”€ spiders/
â”‚   â”œâ”€â”€ animeplanet_spider.py
â”‚   â”œâ”€â”€ drop_collections.py
â”‚   â”œâ”€â”€ mal_spider.py
â”‚   â”œâ”€â”€ mangaupdates_spider.py
â”‚   â”œâ”€â”€ review_spider.py
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ anti_blocking.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ db.py
â”‚   â”‚   â”œâ”€â”€ http.py
â”‚   â”‚   â”œâ”€â”€ io.py
â”‚   â”‚   â””â”€â”€ util.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ enrich_links.py
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ anilist.py
â”‚   â”‚   â”œâ”€â”€ anilist_fetcher.py
â”‚   â”‚   â”œâ”€â”€ animeplanet.py
â”‚   â”‚   â”œâ”€â”€ animeplanet_fetcher.py
â”‚   â”‚   â”œâ”€â”€ animeplanet_fetcher_enhanced.py
â”‚   â”‚   â”œâ”€â”€ mal.py
â”‚   â”‚   â”œâ”€â”€ mal_fetcher.py
â”‚   â”‚   â”œâ”€â”€ mangaupdates.py
â”‚   â”‚   â””â”€â”€ mangaupdates_fetcher.py
â”‚   â”œâ”€â”€ http_client.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ pipeline_conservative.py
â”‚   â”œâ”€â”€ run.py
â”‚   â”œâ”€â”€ scrapy_runner.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ test.py
â”œâ”€â”€ test_mal_performance.py
â”œâ”€â”€ tmp/
â”‚   â””â”€â”€ mal_data/
â””â”€â”€ verify_mal_optimizations.py


=== Folder: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites ===

--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\clear_collections.py ---
from pymongo import MongoClient

def clear_test_collections():
    client = MongoClient("mongodb://localhost:27017")
    db = client["manga_raw_data"]

    collections = [
        "anilist_data",
        "mal_data",
        "mangaupdates_data",
        "animeplanet_data",
    ]

    for col in collections:
        if col in db.list_collection_names():
            db[col].drop()
            print(f"âœ… Dropped collection: {col}")
        else:
            print(f"âš ï¸ Collection not found, skipped: {col}")

    client.close()


if __name__ == "__main__":
    clear_test_collections()
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\debug_animeplanet_enhanced.py ---
# debug_animeplanet_enhanced.py
# Script Ä‘á»ƒ test trá»±c tiáº¿p anime-planet fetcher

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import logging
from pprint import pprint

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def test_single_manga(slug):
    """Test má»™t manga cá»¥ thá»ƒ"""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª Testing anime-planet slug: {slug}")
    print(f"{'='*60}")
    
    # Import the enhanced fetcher
    from src.extractors.animeplanet_fetcher_enhanced import get_full_data
    
    try:
        result = get_full_data(slug)
        
        print(f"\nğŸ“Š RESULTS for {slug}:")
        print(f"Status: {result.get('status')}")
        print(f"HTTP: {result.get('http')}")
        
        # Main info
        main = result.get('main', {})
        print(f"\nğŸ“– Main Info:")
        print(f"  Title: {main.get('title', 'N/A')}")
        print(f"  Synopsis: {len(main.get('synopsis', ''))} chars")
        print(f"  Rating: {main.get('rating', 'N/A')}")
        
        # Reviews
        reviews = result.get('reviews', [])
        print(f"\nğŸ“ Reviews: {len(reviews)} found")
        for i, review in enumerate(reviews[:3]):  # Show first 3
            print(f"  Review {i+1}:")
            print(f"    User: {review.get('user', 'Anonymous')}")
            print(f"    Text: {review.get('text', '')[:100]}...")
        
        # Recommendations  
        recs = result.get('recommendations', [])
        print(f"\nğŸ”— Recommendations: {len(recs)} found")
        for i, rec in enumerate(recs[:5]):  # Show first 5
            print(f"  Rec {i+1}: {rec.get('slug')} - {rec.get('title', 'No title')}")
        
        return result
        
    except Exception as e:
        print(f"âŒ Error testing {slug}: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_multiple_slugs():
    """Test multiple popular manga slugs"""
    test_slugs = [
        "naruto",           # Very popular, should have reviews
        "one-piece",        # Extremely popular  
        "attack-on-titan",  # Popular, should have data
        "tower-of-god",     # Webtoon, moderate popularity
        "berserk"           # Classic, should have reviews
    ]
    
    results = {}
    
    for slug in test_slugs:
        result = test_single_manga(slug)
        if result:
            results[slug] = {
                'status': result.get('status'),
                'reviews_count': len(result.get('reviews', [])),
                'recs_count': len(result.get('recommendations', [])),
                'has_main': bool(result.get('main', {}))
            }
        
        print(f"\nâ³ Waiting before next test...")
        import time
        time.sleep(15)  # Wait between tests
    
    print(f"\n{'='*60}")
    print("ğŸ“ˆ SUMMARY OF ALL TESTS")
    print(f"{'='*60}")
    
    for slug, data in results.items():
        print(f"{slug:20} | Status: {data['status']:12} | Reviews: {data['reviews_count']:3} | Recs: {data['recs_count']:3} | Main: {data['has_main']}")

def test_direct_requests():
    """Test direct HTTP requests Ä‘á»ƒ xem cÃ³ bá»‹ block khÃ´ng"""
    import requests
    import random
    
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ]
    
    test_urls = [
        "https://www.anime-planet.com/manga/naruto",
        "https://www.anime-planet.com/manga/naruto/reviews",
        "https://www.anime-planet.com/manga/naruto/recommendations"
    ]
    
    print(f"\n{'='*60}")
    print("ğŸ” TESTING DIRECT HTTP REQUESTS")
    print(f"{'='*60}")
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    })
    
    for url in test_urls:
        try:
            print(f"\nğŸŒ Testing: {url}")
            resp = session.get(url, timeout=30)
            
            print(f"  Status: {resp.status_code}")
            print(f"  Content-Length: {len(resp.text)}")
            print(f"  Title in HTML: {'<title>' in resp.text}")
            
            # Check for common blocking indicators
            blocked_indicators = [
                "access denied", "blocked", "captcha", "cloudflare", 
                "please enable javascript", "rate limited"
            ]
            
            content_lower = resp.text.lower()
            blocked = any(indicator in content_lower for indicator in blocked_indicators)
            
            if blocked:
                print(f"  âš ï¸ Possible blocking detected")
            else:
                print(f"  âœ… Looks normal")
                
            # Show first 500 chars
            print(f"  Preview: {resp.text[:500]}...")
            
        except Exception as e:
            print(f"  âŒ Error: {e}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        # Test specific slug
        slug = sys.argv[1]
        test_single_manga(slug)
    else:
        print("ğŸš€ Starting comprehensive Anime-Planet debug session")
        
        # 1. Test direct requests first
        test_direct_requests()
        
        # 2. Test with enhanced fetcher
        print(f"\nâ³ Waiting 30 seconds before enhanced tests...")
        import time
        time.sleep(30)
        
        test_multiple_slugs()
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\dump.py ---
import os

OUTPUT_FILE = "project_dump.txt"

def should_ignore_file(filename: str) -> bool:
    """Bá» qua file khÃ´ng cáº§n thiáº¿t"""
    if filename.startswith(".env"):  # Bá» qua má»i file .env*
        return True
    if filename.endswith(".exe"):
        return True
    if filename == "__init__.py":
        return True
    return False

def should_ignore_dir(dirname: str) -> bool:
    """Bá» qua thÆ° má»¥c khÃ´ng quan trá»ng"""
    if dirname.startswith(".env"):  # Bá» qua má»i folder .env*
        return True
    if dirname in ("__pycache__", ".venv", "build", "dist"):
        return True
    return False

def build_tree(root_dir: str, prefix: str = "") -> str:
    """Táº¡o cÃ¢y thÆ° má»¥c giá»‘ng lá»‡nh `tree` (lá»c theo quy táº¯c ignore)."""
    entries = []
    with os.scandir(root_dir) as it:
        for entry in sorted(it, key=lambda e: e.name):
            if entry.is_dir() and not should_ignore_dir(entry.name):
                entries.append(entry)
            elif entry.is_file():
                if should_ignore_file(entry.name):
                    continue
                if entry.name.endswith(".py"):
                    entries.append(entry)

    lines = []
    for i, entry in enumerate(entries):
        connector = "â””â”€â”€ " if i == len(entries) - 1 else "â”œâ”€â”€ "
        if entry.is_dir():
            lines.append(prefix + connector + entry.name + "/")
            extension = "    " if i == len(entries) - 1 else "â”‚   "
            lines.extend(build_tree(entry.path, prefix + extension).splitlines())
        else:
            lines.append(prefix + connector + entry.name)
    return "\n".join(lines)

def dump_project(root_dir: str, output_file: str):
    with open(output_file, "w", encoding="utf-8") as out:
        # In cáº¥u trÃºc thÆ° má»¥c
        out.write("=== Project Tree ===\n")
        out.write(root_dir + "/\n")
        out.write(build_tree(root_dir))
        out.write("\n\n")

        # In ná»™i dung code
        for dirpath, dirnames, filenames in os.walk(root_dir):
            # Lá»c thÆ° má»¥c
            dirnames[:] = [d for d in dirnames if not should_ignore_dir(d)]

            rel_path = os.path.relpath(dirpath, root_dir)
            if rel_path == ".":
                rel_path = ""
            out.write(f"\n=== Folder: {rel_path or root_dir} ===\n")

            for filename in filenames:
                if should_ignore_file(filename):
                    continue
                if filename.endswith(".py"):
                    file_path = os.path.join(dirpath, filename)
                    out.write(f"\n--- File: {file_path} ---\n")
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            out.write(f.read())
                    except Exception as e:
                        out.write(f"[Lá»—i Ä‘á»c file: {e}]\n")

if __name__ == "__main__":
    current_dir = os.getcwd()
    dump_project(current_dir, OUTPUT_FILE)
    print(f"âœ… ÄÃ£ xuáº¥t toÃ n bá»™ code .py (Ä‘Ã£ lá»c .env*, .exe, __pycache__, __init__.py...) + cÃ¢y thÆ° má»¥c vÃ o {OUTPUT_FILE}")
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\run_conservative.py ---
# run_conservative.py
"""
Runner script for conservative pipeline - designed for anime-planet success
"""
import argparse
import logging
import sys
import os

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.pipeline_conservative import run_conservative_pipeline

def main():
    parser = argparse.ArgumentParser(
        description="Conservative manga data fetcher - better for anime-planet"
    )
    parser.add_argument("--limit", type=int, default=5, 
                       help="Number of documents to process (default: 5)")
    parser.add_argument("--skip", type=int, default=0, 
                       help="Number of documents to skip")
    parser.add_argument("--only", nargs="*", 
                       choices=["mal", "anilist", "mangaupdates", "animeplanet"],
                       help="Only run for specific sources")
    parser.add_argument("--animeplanet-only", action="store_true",
                       help="Only run anime-planet (shortcut)")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Setup logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('manga_fetch_conservative.log')
        ]
    )
    
    logger = logging.getLogger(__name__)
    
    # Determine sources to run
    if args.animeplanet_only:
        only_sources = ["animeplanet"]
    else:
        only_sources = args.only
    
    logger.info(f"ğŸš€ Starting conservative pipeline")
    logger.info(f"ğŸ“‹ Settings: limit={args.limit}, skip={args.skip}, only={only_sources}")
    
    if only_sources and "animeplanet" in only_sources:
        logger.warning("âš ï¸ Running anime-planet - this will be SLOW but more reliable")
        logger.warning("âš ï¸ Expected time: ~30-60 seconds per manga")
    
    try:
        results = run_conservative_pipeline(
            limit=args.limit, 
            skip=args.skip, 
            only=only_sources
        )
        
        print(f"\n{'='*80}")
        print("ğŸ“Š CONSERVATIVE PIPELINE RESULTS")  
        print(f"{'='*80}")
        
        # Group results by source
        by_source = {}
        for result in results:
            source = result.get("source", "unknown")
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(result)
        
        # Print summary by source
        for source, source_results in by_source.items():
            print(f"\nğŸ”¹ {source.upper()}:")
            
            success_count = 0
            total_reviews = 0
            total_recs = 0
            
            for result in source_results:
                _id = result.get("_id", "unknown")
                status = result.get("status", "unknown")
                reviews = len(result.get("reviews", []))
                recs = len(result.get("recommendations", []))
                
                # Success if we got any data
                if status in ["ok", "no_reviews"] or reviews > 0 or recs > 0:
                    success_count += 1
                    
                total_reviews += reviews
                total_recs += recs
                
                # Status emoji
                if status == "ok":
                    emoji = "âœ…" 
                elif status == "no_reviews":
                    emoji = "âš ï¸"
                elif status == "error":
                    emoji = "âŒ"
                else:
                    emoji = "â“"
                    
                print(f"  {emoji} {_id:25} | R:{reviews:3} | Rec:{recs:3} | {status}")
            
            success_rate = (success_count / len(source_results)) * 100 if source_results else 0
            print(f"  ğŸ“ˆ Success: {success_count}/{len(source_results)} ({success_rate:.1f}%)")
            print(f"  ğŸ“Š Total: {total_reviews} reviews, {total_recs} recommendations")
        
        print(f"\nğŸ‰ Total results: {len(results)}")
        
        # Special anime-planet analysis
        ap_results = by_source.get("animeplanet", [])
        if ap_results:
            print(f"\n{'='*40}")
            print("ğŸ¯ ANIME-PLANET DETAILED ANALYSIS")
            print(f"{'='*40}")
            
            working_count = 0
            for result in ap_results:
                reviews = len(result.get("reviews", []))
                recs = len(result.get("recommendations", []))
                main = result.get("main", {})
                has_main = bool(main.get("title") or main.get("synopsis"))
                
                if reviews > 0 or recs > 0 or has_main:
                    working_count += 1
                    
                print(f"  {result.get('source_id', 'unknown'):20} | "
                      f"Reviews:{reviews:3} | Recs:{recs:3} | "
                      f"Main:{has_main} | Method:{result.get('method', 'unknown')}")
            
            print(f"\n  ğŸ¯ Anime-Planet working rate: {working_count}/{len(ap_results)} "
                  f"({(working_count/len(ap_results)*100):.1f}%)")
    
    except KeyboardInterrupt:
        logger.info("âŒ Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Pipeline failed: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\test.py ---
import requests

url = "https://www.anime-planet.com/manga/tower-of-god/recommendations"

response = requests.get(url)
html = response.text

with open("recommendations.html", "w", encoding="utf-8") as f:
    f.write(html)

print("Saved recommendations.html (length:", len(html), ")")
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\test_mal_performance.py ---
#!/usr/bin/env python3
"""
Test script to verify MAL fetcher performance and data completeness
"""
import time
from src.extractors.mal_fetcher import MALFetcher

def test_mal_performance():
    """Test MAL fetcher with a known manga ID"""
    fetcher = MALFetcher()
    
    # Test with Monster (MAL ID: 1) - known to have many reviews
    test_id = "1"
    print(f"Testing MAL fetcher with ID: {test_id}")
    
    start_time = time.time()
    result = fetcher.fetch_manga_data(test_id)
    end_time = time.time()
    
    fetch_time = end_time - start_time
    
    print(f"\n=== MAL Fetcher Performance Test ===")
    print(f"Fetch time: {fetch_time:.2f} seconds")
    print(f"Data keys collected: {list(result.keys())}")
    
    # Check reviews
    reviews = result.get('reviews', [])
    print(f"Reviews collected: {len(reviews)}")
    if reviews:
        print(f"First review preview: {reviews[0].get('text', '')[:100]}...")
    
    # Check recommendations
    recommendations = result.get('recommendations', [])
    print(f"Recommendations collected: {len(recommendations)}")
    
    # Check main manga info
    manga_info = result.get('manga_info', {})
    print(f"Main info fields: {len(manga_info)} fields")
    print(f"Title: {manga_info.get('title', 'N/A')}")
    print(f"Score: {manga_info.get('score', 'N/A')}")
    
    return result

if __name__ == "__main__":
    test_mal_performance()

--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\verify_mal_optimizations.py ---
#!/usr/bin/env python3
"""
Verify MAL fetcher optimizations and performance improvements
"""
import time
import logging
from src.extractors.mal_fetcher import MALFetcher

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_mal_optimization():
    """Test MAL fetcher with known manga IDs to verify optimizations"""
    fetcher = MALFetcher()
    
    # Test with multiple known manga IDs
    test_cases = [
        ("1", "Monster"),  # Known to have many reviews
        ("2", "Berserk"),  # Popular manga with reviews
    ]
    
    total_start = time.time()
    results = []
    
    for mal_id, title in test_cases:
        print(f"\n=== Testing {title} (ID: {mal_id}) ===")
        
        start_time = time.time()
        try:
            result = fetcher.fetch_manga_data(mal_id)
            fetch_time = time.time() - start_time
            
            # Analyze results
            reviews = result.get('reviews', [])
            recommendations = result.get('recommendations', [])
            manga_info = result.get('manga_info', {})
            
            test_result = {
                'id': mal_id,
                'title': title,
                'fetch_time': fetch_time,
                'reviews_count': len(reviews),
                'recommendations_count': len(recommendations),
                'data_sections': len(result),
                'success': True
            }
            
            print(f"âœ“ Fetch time: {fetch_time:.2f}s")
            print(f"âœ“ Reviews collected: {len(reviews)}")
            print(f"âœ“ Recommendations: {len(recommendations)}")
            print(f"âœ“ Data sections: {len(result)}")
            
            if reviews:
                print(f"âœ“ Sample review: {reviews[0].get('text', '')[:80]}...")
            
            results.append(test_result)
            
        except Exception as e:
            print(f"âœ— Error: {e}")
            results.append({
                'id': mal_id,
                'title': title,
                'fetch_time': 0,
                'reviews_count': 0,
                'recommendations_count': 0,
                'data_sections': 0,
                'success': False,
                'error': str(e)
            })
    
    total_time = time.time() - total_start
    
    # Summary
    print(f"\n=== Performance Summary ===")
    print(f"Total test time: {total_time:.2f}s")
    print(f"Average time per manga: {total_time/len(test_cases):.2f}s")
    
    successful_tests = [r for r in results if r['success']]
    if successful_tests:
        avg_reviews = sum(r['reviews_count'] for r in successful_tests) / len(successful_tests)
        avg_recommendations = sum(r['recommendations_count'] for r in successful_tests) / len(successful_tests)
        print(f"Average reviews per manga: {avg_reviews:.1f}")
        print(f"Average recommendations per manga: {avg_recommendations:.1f}")
    
    # Check if optimizations are working
    print(f"\n=== Optimization Check ===")
    for result in results:
        if result['success']:
            if result['reviews_count'] > 3:
                print(f"âœ“ {result['title']}: Review collection improved (>{result['reviews_count']} reviews)")
            else:
                print(f"âš  {result['title']}: Limited reviews ({result['reviews_count']})")
            
            if result['fetch_time'] < 10:
                print(f"âœ“ {result['title']}: Good performance ({result['fetch_time']:.2f}s)")
            else:
                print(f"âš  {result['title']}: Slow performance ({result['fetch_time']:.2f}s)")
    
    return results

if __name__ == "__main__":
    test_mal_optimization()

=== Folder: spiders ===

--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\spiders\animeplanet_spider.py ---
# spiders/animeplanet_spider.py
# Scrapy spider => collects details, reviews, recommendations from anime-planet manga page
# Usage:
#   scrapy runspider spiders/animeplanet_spider.py -a slug=tower-of-god

import os
import json
import random
import logging
from datetime import datetime
from urllib.parse import urljoin

import pymongo
import scrapy
from scrapy.crawler import CrawlerProcess

logger = logging.getLogger("animeplanet_spider")

MONGO_URI = os.environ.get("MONGO_URI", "mongodb://localhost:27017")
MONGO_DB = os.environ.get("MONGO_DB", "manga_raw_data")
COLLECTION = "animeplanet_data"

# A reasonably sized UA pool (extend as needed)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; Pixel 7a) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Mobile Safari/537.36",
]

def mongo_client():
    return pymongo.MongoClient(MONGO_URI)


class AnimePlanetSpider(scrapy.Spider):
    name = "animeplanet_spider"
    # Conservative settings to reduce chance of being blocked
    custom_settings = {
        "DOWNLOAD_DELAY": 3.0,
        "CONCURRENT_REQUESTS": 1,
        "RETRY_ENABLED": False,
        "COOKIES_ENABLED": True,
        # ensure default headers (can be overridden per request)
        "DEFAULT_REQUEST_HEADERS": {
            "Accept-Language": "en-US,en;q=0.9",
            "DNT": "1",
            "Referer": "https://www.google.com/",
            "Connection": "keep-alive",
        },
    }

    def __init__(self, slug=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if not slug:
            raise ValueError("Missing required slug argument (e.g. tower-of-god)")
        self.slug = slug.strip().rstrip("/")
        self.start_urls = [f"https://www.anime-planet.com/manga/{self.slug}"]
        self.client = mongo_client()
        self.db = self.client[MONGO_DB]
        self.col = self.db[COLLECTION]
        self.doc_id = f"ap_{self.slug}"

        # proxy from env (optional)
        self.proxy = os.environ.get("HTTP_PROXY") or os.environ.get("HTTPS_PROXY")

    def _headers(self):
        ua = random.choice(USER_AGENTS)
        headers = {
            "User-Agent": ua,
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://www.google.com/",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        return headers

    def start_requests(self):
        for url in self.start_urls:
            meta = {"retries": 0}
            if self.proxy:
                meta["proxy"] = self.proxy
            yield scrapy.Request(url, headers=self._headers(), callback=self.parse_main, errback=self.errback, dont_filter=True, meta=meta)

    def errback(self, failure):
        logger.error("Request failure: %s", failure)
        doc = {
            "_id": self.doc_id,
            "fetched_at": datetime.utcnow().isoformat(),
            "source": "animeplanet",
            "source_id": self.slug,
            "source_url": f"https://www.anime-planet.com/manga/{self.slug}",
            "http": {"error": str(failure.value) if failure.value else "request_failed"},
            "status": "error",
        }
        try:
            self.col.replace_one({"_id": self.doc_id}, doc, upsert=True)
        except Exception:
            logger.exception("Failed to save error doc to mongo", exc_info=True)

    def parse_main(self, response):
        # If blocked (403), retry a few times with different UA and optional proxy
        if response.status == 403:
            retries = response.request.meta.get("retries", 0)
            if retries < 3:
                logger.warning("Received 403 for %s, retrying attempt %d", response.url, retries + 1)
                meta = {"retries": retries + 1}
                if self.proxy:
                    meta["proxy"] = self.proxy
                yield scrapy.Request(response.url, headers=self._headers(), callback=self.parse_main, errback=self.errback, dont_filter=True, meta=meta)
                return
            else:
                logger.warning("403 after retries for %s â€” saving forbidden doc", response.url)
                doc = {
                    "_id": self.doc_id,
                    "fetched_at": datetime.utcnow().isoformat(),
                    "source": "animeplanet",
                    "source_id": self.slug,
                    "source_url": response.url,
                    "http": {"code": 403},
                    "raw_prefix": response.text[:20000] if hasattr(response, "text") else None,
                    "status": "forbidden",
                }
                self.col.replace_one({"_id": self.doc_id}, doc, upsert=True)
                return

        # Collect main info (title, synopsis, rating if available)
        title = response.xpath("//h1/text()").get()
        synopsis = response.xpath("//div[contains(@class,'synopsis')]/p//text()").getall()
        synopsis = " ".join([s.strip() for s in synopsis]).strip()
        if not synopsis:
            synopsis = response.xpath("//meta[@name='description']/@content").get() or ""

        rating = response.xpath("//div[contains(@class,'avgRating') or contains(@class,'rating')]/text()").get()
        if rating:
            rating = rating.strip()

        recs = []
        rec_selectors = [
            "//section[contains(. , 'Similar') or contains(. , 'recommend')]/.//a[contains(@href,'/manga/')]/@href",
            "//a[contains(@class,'similar') and contains(@href,'/manga/')]/@href",
            "//div[contains(@class,'recommendations')]//a[contains(@href,'/manga/')]/@href",
            "//a[contains(@href,'/manga/') and contains(@class,'media')]/@href",
        ]
        for sel in rec_selectors:
            items = response.xpath(sel).getall()
            if items:
                for href in items:
                    href = href.strip()
                    if href.startswith("/"):
                        href = urljoin("https://www.anime-planet.com", href)
                    if "/manga/" in href:
                        slug = href.split("/manga/")[-1].split("?")[0].split("#")[0].strip("/")
                    else:
                        slug = href
                    recs.append({"slug": slug, "url": href})
                break

        # request reviews page
        reviews_url = response.url.rstrip("/") + "/reviews"
        meta = {"main": {"title": title, "synopsis": synopsis, "rating": rating, "recs": recs}, "retries": 0}
        if self.proxy:
            meta["proxy"] = self.proxy
        yield scrapy.Request(reviews_url, headers=self._headers(), callback=self.parse_reviews, meta=meta, dont_filter=True, errback=self.errback)

    def parse_reviews(self, response):
        main = response.meta.get("main", {})
        if response.status == 403:
            retries = response.request.meta.get("retries", 0)
            if retries < 3:
                logger.warning("Reviews page 403 for %s, retry attempt %d", response.url, retries + 1)
                meta = response.request.meta.copy()
                meta["retries"] = retries + 1
                if self.proxy:
                    meta["proxy"] = self.proxy
                yield scrapy.Request(response.url, headers=self._headers(), callback=self.parse_reviews, meta=meta, dont_filter=True, errback=self.errback)
                return
            else:
                logger.warning("Reviews page forbidden after retries %s", response.url)
                doc = {
                    "_id": self.doc_id,
                    "fetched_at": datetime.utcnow().isoformat(),
                    "source": "animeplanet",
                    "source_id": self.slug,
                    "source_url": f"https://www.anime-planet.com/manga/{self.slug}",
                    "http": {"code": 403},
                    "raw_prefix": response.text[:20000] if hasattr(response, "text") else None,
                    "status": "forbidden_reviews",
                }
                self.col.replace_one({"_id": self.doc_id}, doc, upsert=True)
                return

        reviews = []
        review_nodes = response.xpath("//div[contains(@class,'user-review') or contains(@class,'review') or //article[contains(@class,'review')]]")
        if not review_nodes:
            review_nodes = response.xpath("//li[contains(@class,'review') or contains(@class,'comment')]")
        for node in review_nodes:
            user = node.xpath(".//a[contains(@href,'/user/')]/text() | .//h3//text()").get()
            score = node.xpath(".//span[contains(@class,'score')]/text() | .//div[contains(@class,'rating')]/text()").get()
            parts = node.xpath(".//div[contains(@class,'review-body')]//text() | .//p//text()").getall()
            content = " ".join([p.strip() for p in parts]).strip()
            date = node.xpath(".//time/@datetime | .//span[contains(@class,'date')]/text() | .//time/text()").get()
            reviews.append({"user": user.strip() if user else None, "score": score.strip() if score else None, "content": content, "date": date})

        # try JSON-LD fallback
        if not reviews:
            ld = response.xpath("//script[@type='application/ld+json']/text()").get()
            if ld:
                try:
                    parsed = json.loads(ld)
                    if isinstance(parsed, dict) and parsed.get("review"):
                        rv = parsed.get("review")
                        if isinstance(rv, list):
                            for r in rv:
                                reviews.append({
                                    "user": r.get("author", {}).get("name") if isinstance(r.get("author"), dict) else r.get("author"),
                                    "score": r.get("reviewRating", {}).get("ratingValue") if r.get("reviewRating") else None,
                                    "content": r.get("reviewBody"),
                                    "date": r.get("datePublished"),
                                })
                except Exception:
                    logger.debug("Failed parse ld+json on %s", response.url, exc_info=True)

        doc = {
            "_id": self.doc_id,
            "fetched_at": datetime.utcnow().isoformat(),
            "source": "animeplanet",
            "source_id": self.slug,
            "source_url": f"https://www.anime-planet.com/manga/{self.slug}",
            "status": "ok" if reviews or main.get("recs") else ("empty" if not reviews and not main.get("recs") else "partial"),
            "main": {
                "title": main.get("title"),
                "synopsis": main.get("synopsis"),
                "rating": main.get("rating"),
                "recommendations": main.get("recs"),
            },
            "reviews": reviews,
            "raw_prefix": response.text[:20000] if hasattr(response, "text") else None
        }
        self.col.replace_one({"_id": self.doc_id}, doc, upsert=True)
        logger.info("[SAVED] %s %s | reviews=%d recs=%d status=%s", COLLECTION, self.doc_id, len(reviews), len(main.get("recs", [])), doc["status"])


if __name__ == "__main__":
    slug = os.environ.get("AP_SLUG", "tower-of-god")
    process = CrawlerProcess(settings={})
    process.crawl(AnimePlanetSpider, slug=slug)
    process.start()
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\spiders\drop_collections.py ---
# scripts/drop_collections.py
# simple script to drop the 4 collections used for testing
# Usage:
#   python scripts/drop_collections.py
# or set MONGO_URI / MONGO_DB env vars if not default

import os
import pymongo

MONGO_URI = os.environ.get("MONGO_URI", "mongodb://localhost:27017")
MONGO_DB = os.environ.get("MONGO_DB", "manga_raw_data")

COLLECTIONS = ["animeplanet_data", "mangaupdates_data", "mal_data", "anilist_data"]

def main():
    client = pymongo.MongoClient(MONGO_URI)
    db = client[MONGO_DB]
    for c in COLLECTIONS:
        if c in db.list_collection_names():
            db.drop_collection(c)
            print(f"Dropped collection: {c}")
        else:
            print(f"Collection not found (skipped): {c}")

if __name__ == "__main__":
    main()
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\spiders\mal_spider.py ---
import scrapy
from scrapy.http import Request
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class MalSpider(scrapy.Spider):
    name = "mal_spider"
    allowed_domains = ["myanimelist.net"]
    custom_settings = {
        'DOWNLOAD_DELAY': 2,
        'CONCURRENT_REQUESTS': 4,
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    }

    def __init__(self, mal_id=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.mal_id = mal_id
        self.start_urls = [f"https://myanimelist.net/manga/{mal_id}"]
        self.payload = {
            "_id": f"mal_{mal_id}",
            "source": "mal",
            "source_id": mal_id,
            "source_url": f"https://myanimelist.net/manga/{mal_id}",
            "fetched_at": datetime.utcnow().isoformat(),
            "manga_info": {},
            "recommendations": [],
            "reviews": [],
            "status": "ok",
            "http": {"code": 200},
        }

    def parse(self, response):
        # Parse manga info
        info = {}
        info['jpName'] = response.xpath('//span[contains(text(), "Japanese:")]/following::text()').get(default='').strip()
        info['engName'] = response.xpath('//span[contains(text(), "English:")]/following::text()').get(default='').strip()
        info['synonyms'] = response.xpath('//span[contains(text(), "Synonyms:")]/following::text()').get(default='').strip()
        info['type'] = response.xpath('//span[text()="Type:"]/following-sibling::a/text()').get(default='')
        info['volumes'] = response.xpath('//span[text()="Volumes:"]/following::text()').get(default='').strip()
        info['chapters'] = response.xpath('//span[text()="Chapters:"]/following::text()').get(default='').strip()
        info['status'] = response.xpath('//span[text()="Status:"]/following::text()').get(default='').strip()
        info['published'] = response.xpath('//span[text()="Published:"]/following::text()').get(default='').strip()
        info['genres'] = ', '.join(response.xpath('//span[text()="Genres:"]/following-sibling::a/text()').getall())
        info['themes'] = ', '.join(response.xpath('//span[text()="Themes:"]/following-sibling::a/text()').getall())
        info['demographic'] = response.xpath('//span[text()="Demographic:"]/following-sibling::a/text()').get(default='')
        info['serialization'] = ', '.join(response.xpath('//span[text()="Serialization:"]/following-sibling::a/text()').getall())
        info['authors'] = ', '.join(response.xpath('//span[text()="Authors:"]/following-sibling::a/text()').getall())
        info['score'] = response.css('span.score-label::text').get(default='')
        info['ranked'] = response.xpath('//span[text()="Ranked:"]/following::text()').get(default='').strip()
        info['popularity'] = response.xpath('//span[text()="Popularity:"]/following::text()').get(default='').strip()
        info['members'] = response.xpath('//span[text()="Members:"]/following::text()').get(default='').strip()
        info['favorites'] = response.xpath('//span[text()="Favorites:"]/following::text()').get(default='').strip()
        info['cover_image'] = response.css('div.leftside img.lazyload::attr(src)').get(default=response.css('div.leftside img.lazyload::attr(data-src)').get(default=''))
        info['synopsis'] = response.xpath('//span[@itemprop="description"]/text()').get(default='').strip()
        
        self.payload["manga_info"] = info
        logger.info(f"Parsed manga info for {self.mal_id}: {bool(info)}")
        
        # Fetch recommendations
        yield Request(
            f"https://myanimelist.net/manga/{self.mal_id}/_/userrecs",
            callback=self.parse_recommendations,
            meta={'mal_id': self.mal_id}
        )
        
        # Fetch reviews
        yield Request(
            f"https://myanimelist.net/manga/{self.mal_id}/reviews?p=1",
            callback=self.parse_reviews,
            meta={'mal_id': self.mal_id, 'page': 1}
        )

    def parse_recommendations(self, response):
        recs = []
        for a in response.css('div.borderClass a[href*="/manga/"]'):
            href = a.attrib.get('href', '')
            title = a.css('::text').get(default='').strip()
            if "/manga/" not in href or not title or len(title) < 2:
                continue
            try:
                mid = href.split("/manga/")[1].split("/")[0]
                if mid.isdigit():
                    reason = a.xpath('following-sibling::div/text()').get(default='').strip()[:200]
                    recs.append({
                        "id": mid,
                        "title": title,
                        "url": href,
                        "reason": reason
                    })
            except Exception:
                continue
        
        seen = set()
        unique_recs = [rec for rec in recs if rec["id"] not in seen and not seen.add(rec["id"])]
        self.payload["recommendations"] = unique_recs[:20]
        logger.info(f"Parsed {len(unique_recs)} recommendations for {self.mal_id}")

    def parse_reviews(self, response):
        mal_id = response.meta['mal_id']
        page = response.meta['page']
        
        reviews = []
        for review in response.css('div.review-element, div.review-element.js-review-element, div.borderDark'):
            try:
                review_id = review.css('div.open a::attr(href), a[href*="/reviews/"]::attr(href)').get(default='').split('/')[-1]
                if not review_id:
                    logger.debug(f"No review ID found for {mal_id} on page {page}")
                    continue
                
                review_text = ' '.join(review.css('div.text, div.review-body').get(default='').strip().split())
                if not review_text or len(review_text) < 5:
                    logger.debug(f"Skipping short review for {mal_id}: {review_text[:50]}...")
                    continue
                
                reactions_dict = review.attrib.get('data-reactions', '')
                reactions = {}
                if reactions_dict:
                    try:
                        import json
                        reactions_data = json.loads(reactions_dict)
                        reaction_type_map = ['nice', 'loveIt', 'funny', 'confusing', 'informative', 'wellWritten', 'creative']
                        reactions = {r: c for r, c in zip(reaction_type_map, reactions_data.get('count', ['0']*7))}
                    except:
                        logger.debug(f"Error parsing reactions for {mal_id}: {reactions_dict}")
                
                author = review.css('div.username a::text, div.reviewer a::text').get(default='').strip()
                score = review.css('div.rating span.num::text, div.score::text').get(default='').strip()
                post_time = review.css('div.update_at::text, div.date::text').get(default='').strip()
                episodes_seen = review.css('.tag.preliminary span::text, div.episodes-seen::text').get(default='').strip()
                recommendation_status = review.css('.tag.recommended::text, .tag.recommendation::text').get(default='').strip()
                profile_url = review.css('div.thumb a::attr(href), div.reviewer a::attr(href)').get(default='')
                profile_img = review.css('div.thumb a img::attr(src), div.reviewer img::attr(src)').get(default='')
                
                reviews.append({
                    'reviewId': review_id,
                    'text': review_text[:3000],
                    'author': author,
                    'score': score,
                    'postTime': post_time,
                    'episodesSeen': episodes_seen,
                    'recommendationStatus': recommendation_status,
                    'profileUrl': profile_url,
                    'profileImage': profile_img,
                    **reactions
                })
            except Exception as e:
                logger.debug(f"Error parsing review for {mal_id}: {e}")
                continue
        
        self.payload["reviews"].extend(reviews)
        logger.info(f"Parsed {len(reviews)} reviews for {mal_id} on page {page}")
        
        # Check for next page
        next_page_url = response.css('a[href*="reviews?p="]:not([href*="p=1"])::attr(href), div.mt4 a[href*="reviews?p="]::attr(href)').get()
        if next_page_url:
            if not next_page_url.startswith('http'):
                next_page_url = f"https://myanimelist.net{next_page_url}"
            logger.debug(f"Found next page for {mal_id}: {next_page_url}")
            yield Request(
                next_page_url,
                callback=self.parse_reviews,
                meta={'mal_id': mal_id, 'page': page + 1}
            )
        else:
            logger.info(f"No next page for reviews of {mal_id} after page {page}")
            yield self.payload

    def closed(self, reason):
        logger.info(f"Spider closed for {self.mal_id}: {reason}")
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\spiders\mangaupdates_spider.py ---
# spiders/mangaupdates_spider.py
# Scrapy spider => collects comments (reviews) from MangaUpdates series page
# Example usage:
#   scrapy runspider spiders/mangaupdates_spider.py -a mu_url="https://www.mangaupdates.com/series/...#comments"

import os
import random
import logging
from datetime import datetime
from urllib.parse import urlparse

import pymongo
import scrapy
from scrapy.crawler import CrawlerProcess

logger = logging.getLogger("mangaupdates_spider")

MONGO_URI = os.environ.get("MONGO_URI", "mongodb://localhost:27017")
MONGO_DB = os.environ.get("MONGO_DB", "manga_raw_data")
COLLECTION = "mangaupdates_data"

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36",
]

def mongo_client():
    return pymongo.MongoClient(MONGO_URI)


class MangaUpdatesSpider(scrapy.Spider):
    name = "mangaupdates_spider"
    custom_settings = {
        "DOWNLOAD_DELAY": 2.0,
        "CONCURRENT_REQUESTS": 2,
        "RETRY_ENABLED": False,
        "COOKIES_ENABLED": True,
    }

    def __init__(self, mu_id=None, mu_url=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if not (mu_id or mu_url):
            raise ValueError("Provide mu_id or mu_url to spider")
        self.mu_id = str(mu_id) if mu_id else None
        self.mu_url = mu_url
        self.client = mongo_client()
        self.db = self.client[MONGO_DB]
        self.col = self.db[COLLECTION]
        if self.mu_id:
            self.doc_id = f"mu_{self.mu_id}"
        else:
            path = urlparse(self.mu_url).path.strip("/").replace("/", "_")
            self.doc_id = f"mu_{path}"
        if self.mu_url:
            self.start_urls = [self.mu_url]
        else:
            self.start_urls = []

    def start_requests(self):
        if not self.start_urls:
            logger.error("No start_urls for MangaUpdatesSpider. Provide mu_url.")
            return
        for url in self.start_urls:
            headers = {"User-Agent": random.choice(USER_AGENTS), "Accept-Language": "en-US,en;q=0.9"}
            parsed = url.split("?")[0]
            base_with_comments = parsed + "?perpage=100&page=1#comments"
            yield scrapy.Request(base_with_comments, headers=headers, callback=self.parse_comments, meta={"page": 1, "base": parsed}, dont_filter=True)

    def parse_comments(self, response):
        if response.status == 403:
            logger.warning("403 on MangaUpdates %s", response.url)
            doc = {
                "_id": self.doc_id,
                "fetched_at": datetime.utcnow().isoformat(),
                "source": "mangaupdates",
                "source_url": self.start_urls[0] if self.start_urls else None,
                "http": {"code": 403},
                "status": "forbidden",
                "raw_prefix": response.text[:20000] if hasattr(response, "text") else None,
            }
            self.col.replace_one({"_id": self.doc_id}, doc, upsert=True)
            return

        page = response.meta.get("page", 1)
        base = response.meta.get("base", response.url.split("?")[0])
        comments = []

        # Try multiple selectors (existing heuristics kept)
        comment_nodes = response.xpath("//div[@id='comments']//div[contains(@class,'postbody')] | //div[@id='comments']//li[contains(@class,'comment') or contains(@class,'post')]")
        if not comment_nodes:
            comment_nodes = response.xpath("//div[contains(@class,'post') and contains(@class,'postbody')] | //li[contains(@class,'comment')]")

        for node in comment_nodes:
            user = node.xpath(".//a[contains(@href,'member.php')]/text() | .//a[contains(@href,'members')]/text() | .//span[@class='username']//text()").get()
            content_parts = node.xpath(".//div[contains(@class,'postbody')]//text() | .//p//text()").getall()
            content = " ".join([c.strip() for c in content_parts]).strip()
            date = node.xpath(".//span[contains(@class,'date')]/text() | .//div[contains(@class,'postdate')]//text() | .//abbr[@class='published']/@title").get()
            comments.append({"user": user.strip() if user else None, "content": content, "date": date})

        existing = self.col.find_one({"_id": self.doc_id}) or {}
        all_comments = existing.get("comments", [])
        existing_texts = {c.get("content") for c in all_comments}
        new_added = 0
        for c in comments:
            if c.get("content") and c.get("content") not in existing_texts:
                all_comments.append(c)
                new_added += 1

        doc = {
            "_id": self.doc_id,
            "fetched_at": datetime.utcnow().isoformat(),
            "source": "mangaupdates",
            "source_url": base,
            "page_last_fetched": page,
            "comments": all_comments,
            "status": "ok" if all_comments else "no_comments",
            "raw_prefix": response.text[:20000] if hasattr(response, "text") else None,
        }
        self.col.replace_one({"_id": self.doc_id}, doc, upsert=True)
        logger.info("[SAVED] %s %s | page=%d comments_page=%d total=%d", COLLECTION, self.doc_id, page, len(comments), len(all_comments))

        # determine next page
        next_page = page + 1
        next_url = f"{base}?perpage=100&page={next_page}#comments"
        if comments and len(comments) >= 100:
            yield scrapy.Request(next_url, headers={"User-Agent": random.choice(USER_AGENTS)}, callback=self.parse_comments, meta={"page": next_page, "base": base}, dont_filter=True)
        else:
            logger.info("No more comment pages or last page detected for %s", base)


if __name__ == "__main__":
    mu_url = os.environ.get("MU_URL")
    if not mu_url:
        print("Set MU_URL environment variable to a mangaupdates series page URL to test directly.")
    else:
        process = CrawlerProcess(settings={})
        process.crawl(MangaUpdatesSpider, mu_url=mu_url)
        process.start()

--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\spiders\review_spider.py ---
import scrapy
import re


class ReviewSpider(scrapy.Spider):
    name = "review_spider"

    def __init__(self, mal_id=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if not mal_id:
            raise ValueError("mal_id is required. Example: -a mal_id=1")
        self.start_urls = [f"https://myanimelist.net/manga/{mal_id}/_/reviews"]

    def parse(self, response):
        reviews = response.css("div.review-element")

        for r in reviews:
            user = r.css("div.username a::text").get() or r.css("div.username::text").get()
            user = user.strip() if user else None

            score_text = r.css("div.rating span.num::text").get()
            score = int(score_text) if score_text and score_text.isdigit() else None

            date_text = r.css("div.update_at::text").get()
            date = date_text.strip() if date_text else None

            content_parts = r.css("div.text::text, div.text *::text").getall()
            content = " ".join([c.strip() for c in content_parts if c.strip()])

            tags = r.css("div.tags span.tag::text").getall()
            tags = [t.strip() for t in tags if t.strip()]

            yield {
                "user": user,
                "score": score,
                "content": content,
                "date": date,
                "tags": tags,
            }

        next_page = response.css("div.pagination a.next::attr(href)").get()
        if next_page:
            yield response.follow(next_page, callback=self.parse)
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\spiders\settings.py ---
# src/spiders/settings.py

# Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh cho táº¥t cáº£ spiders

DEFAULT_USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/128.0.0.0 Safari/537.36"
)

DEFAULT_HEADERS = {
    "User-Agent": DEFAULT_USER_AGENT,
    "Accept": (
        "text/html,application/xhtml+xml,application/xml;"
        "q=0.9,image/avif,image/webp,*/*;q=0.8"
    ),
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://www.google.com/",
    "DNT": "1",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
}

SPIDER_SETTINGS = {
    "COOKIES_ENABLED": True,
    "DOWNLOAD_DELAY": 1.5,
    "DEFAULT_REQUEST_HEADERS": DEFAULT_HEADERS,
}

# Scrapy settings for manga scraping project

BOT_NAME = 'manga_scraper'

SPIDER_MODULES = ['spiders']
NEWSPIDER_MODULE = 'spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Configure maximum concurrent requests
CONCURRENT_REQUESTS = 128
CONCURRENT_REQUESTS_PER_DOMAIN = 64
DOWNLOAD_DELAY = 0.1

# Enable autothrottle
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 0.1
AUTOTHROTTLE_MAX_DELAY = 5.0
AUTOTHROTTLE_TARGET_CONCURRENCY = 8.0

# Retry settings
RETRY_HTTP_CODES = [429, 500, 502, 503, 504]
RETRY_TIMES = 10

# Disable cookies
COOKIES_ENABLED = False

# Enable User-Agent rotation
DOWNLOADER_MIDDLEWARES = {
    'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 500,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 350,
}

# Proxy settings (uncomment and configure with your proxy service)
# PROXY_POOL_ENABLED = True
# PROXY_POOL = [
#     'http://proxy1:port',
#     'http://proxy2:port',
# ]

# Default headers
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

# Logging
LOG_ENABLED = True
LOG_LEVEL = 'INFO'

# Export encoding
FEED_EXPORT_ENCODING = 'utf-8'
=== Folder: src ===

--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\config.py ---
import os
from dotenv import load_dotenv

load_dotenv()

MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017/")
MONGO_DB = os.getenv("MONGO_DB", "manga_raw_data")

WORKERS = int(os.getenv("WORKERS", "6"))

AP_MAX_PAGES_REVIEWS = int(os.getenv("AP_MAX_PAGES_REVIEWS", "2"))
AP_MAX_PAGES_RECS = int(os.getenv("AP_MAX_PAGES_RECS", "2"))
MU_MAX_PAGES_COMMENTS = int(os.getenv("MU_MAX_PAGES_COMMENTS", "2"))

MIN_HOST_INTERVAL = float(os.getenv("MIN_HOST_INTERVAL", "0.8"))
HTTP_TIMEOUT = int(os.getenv("HTTP_TIMEOUT", "30"))

# User-Agent chuáº©n tá»­ táº¿ Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n sá»›m
DEFAULT_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/126.0.0.0 Safari/537.36"
)
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\db.py ---
from pymongo import MongoClient
import os

# Mongo connection string (máº·c Ä‘á»‹nh localhost:27017, database = manga_raw_data)
MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017/")

# Táº¡o client global (chá»‰ cáº§n 1 káº¿t ná»‘i cho toÃ n project)
_client = MongoClient(MONGO_URI)


def get_db(db_name: str):
    """Tráº£ vá» database object."""
    return _client[db_name]


def get_collection(db_name: str, col_name: str):
    """Tráº£ vá» collection object."""
    db = get_db(db_name)
    return db[col_name]
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\enrich_links.py ---
from pymongo import MongoClient
from tqdm import tqdm

def enrich_links():
    client = MongoClient("mongodb://localhost:27017")
    db = client["manga_raw_data"]
    col = db["mangadex_manga"]

    cursor = col.find({}, no_cursor_timeout=True)

    for doc in tqdm(cursor, desc="Enriching"):
        links = doc.get("links") or {}
        updates = {}
        if "al" in links:
            updates["anilist_id"] = links["al"]
        if "ap" in links:
            updates["ap_slug"] = links["ap"]
        if "mu" in links:
            updates["mu_id"] = links["mu"]
        if "mal" in links:
            updates["mal_id"] = links["mal"]

        if updates:
            col.update_one({"_id": doc["_id"]}, {"$set": updates})

    cursor.close()
    client.close()

if __name__ == "__main__":
    enrich_links()
    print("âœ… Done enriching links")
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\http_client.py ---
import requests
from tenacity import retry, wait_fixed, stop_after_attempt


class HttpError(Exception):
    pass


HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0 Safari/537.36"
}


@retry(wait=wait_fixed(2), stop=stop_after_attempt(3))
def http_get(url, params=None, headers=None, allow_statuses=None, allow_404=False):
    hdrs = HEADERS.copy()
    if headers:
        hdrs.update(headers)

    resp = requests.get(url, params=params, headers=hdrs, timeout=15)

    # Cho phÃ©p cÃ¡c status code Ä‘áº·c biá»‡t
    if allow_statuses and resp.status_code in allow_statuses:
        return resp
    if allow_404 and resp.status_code == 404:
        return resp

    if resp.status_code != 200:
        raise HttpError(f"GET {url} -> {resp.status_code}")

    return resp


@retry(wait=wait_fixed(2), stop=stop_after_attempt(3))
def http_post(url, json=None, data=None, headers=None, allow_statuses=None, allow_404=False):
    hdrs = HEADERS.copy()
    if headers:
        hdrs.update(headers)

    resp = requests.post(url, json=json, data=data, headers=hdrs, timeout=15)

    if allow_statuses and resp.status_code in allow_statuses:
        return resp
    if allow_404 and resp.status_code == 404:
        return resp

    if resp.status_code != 200:
        raise HttpError(f"POST {url} -> {resp.status_code}")

    return resp
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\pipeline.py ---
# src/pipeline.py
"""
Pipeline runner: read docs from mangadex_manga, resolve external source ids,
call collectors, normalize results and upsert into target collections.
(MODIFIED to run all sources concurrently)
"""

from datetime import datetime
from typing import Optional, Tuple, List, Any, Dict, Callable, Set
import concurrent.futures
import logging

from .db import get_collection
from .extractors.mal import collect_mal
from .extractors.anilist import get_full_data as collect_anilist
from .extractors.animeplanet import collect_animeplanet
from .extractors.mangaupdates import collect_mangaupdates

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# --- CÃ¡c dictionary Ã¡nh xáº¡ khÃ´ng thay Ä‘á»•i ---
LINK_KEY_FOR_SOURCE = {
    "anilist": "al",
    "mal": "mal",
    "mangaupdates": "mu",
    "animeplanet": "ap",
}

TOPLEVEL_FIELD_FOR_SOURCE = {
    "anilist": "anilist_id",
    "mal": "mal_id",
    "mangaupdates": "mu_id",
    "animeplanet": "ap_slug",
}

COL_NAME_FOR_SOURCE = {
    "anilist": "anilist_data",
    "mal": "mal_data",
    "mangaupdates": "mangaupdates_data",
    "animeplanet": "animeplanet_data",
}

PARAM_ALIASES = {
    "anilist": ["al_id", "al", "anilist_id", "source_id"],
    "mal": ["mal_id", "mid", "id", "source_id"],
    "mangaupdates": ["mu_id", "mu", "mu_slug", "source_id"],
    "animeplanet": ["ap_id", "ap", "ap_slug", "animeplanet_id", "source_id"],
}

# --- CÃ¡c hÃ m helper khÃ´ng thay Ä‘á»•i ---
def _doc_id_for_log(doc: dict) -> str:
    return str(doc.get("_id"))

def _resolve_source_id_from_doc(doc: dict, source: str) -> Optional[str]:
    top_field = TOPLEVEL_FIELD_FOR_SOURCE.get(source)
    if top_field:
        v = doc.get(top_field)
        if v:
            logger.debug("%s %s: found top-level %s=%s", _doc_id_for_log(doc), source, top_field, v)
            return str(v)
    attrs = doc.get("attributes") or {}
    links = attrs.get("links") or {}
    link_key = LINK_KEY_FOR_SOURCE.get(source)
    if link_key:
        v = links.get(link_key)
        if v:
            logger.debug("%s %s: found attributes.links.%s=%s", _doc_id_for_log(doc), source, link_key, v)
            return str(v)
    top_links = doc.get("links") or {}
    if link_key:
        v = top_links.get(link_key)
        if v:
            logger.debug("%s %s: found top-level links.%s=%s", _doc_id_for_log(doc), source, link_key, v)
            return str(v)
    v = doc.get(source)
    if v:
        logger.debug("%s %s: found direct top-level %s=%s", _doc_id_for_log(doc), source, source, v)
        return str(v)
    logger.debug("%s %s: nothing found", _doc_id_for_log(doc), source)
    return None

def _try_call_collector(fn: Callable, source: str, mangadex_id: str, source_id: str | List[str]) -> Any:
    try:
        return fn(source_id)
    except TypeError:
        pass
    candidates = [
        {"mangadex_id": mangadex_id, "source_id": source_id},
        {"mangadex_id": mangadex_id},
        {"source_id": source_id},
        {f"{source}_id": source_id},
    ]
    for alias in PARAM_ALIASES.get(source, []):
        candidates.append({alias: source_id})
        candidates.append({"mangadex_id": mangadex_id, alias: source_id})
    for kwargs in candidates:
        try:
            return fn(**kwargs)
        except TypeError:
            continue
    try:
        return fn()
    except TypeError:
        pass
    raise TypeError(f"Could not call collector {fn.__name__} with any known signature for source={source}")

def _normalize_collector_result(source: str, result: Any, source_id: str | List[str]) -> List[Tuple[str, Dict]]:
    if result is None:
        raise ValueError("collector returned None")
    results = []
    if isinstance(result, list):
        for payload in result:
            if not isinstance(payload, dict):
                payload = {"raw": payload}
            payload.setdefault("source", source)
            # Sá»­a lá»—i logic nhá»: source_id cá»§a AniList tráº£ vá» trong payload
            # nÃªn Ä‘Æ°á»£c Æ°u tiÃªn hÆ¡n lÃ  source_id cá»§a batch.
            effective_sid = payload.get("source_id") or payload.get("_id", "").replace("anilist_", "")
            payload["source_id"] = effective_sid
            col_name = COL_NAME_FOR_SOURCE.get(source, f"{source}_data")
            results.append((col_name, payload))
    else:
        if isinstance(result, tuple):
            if len(result) == 2:
                col_name, payload = result
            elif len(result) == 3:
                _, col_name, payload = result
            else:
                col_name = COL_NAME_FOR_SOURCE.get(source, f"{source}_data")
                payload = {"raw": result}
        elif isinstance(result, dict):
            col_name = COL_NAME_FOR_SOURCE.get(source, f"{source}_data")
            payload = result
        else:
            col_name = COL_NAME_FOR_SOURCE.get(source, f"{source}_data")
            payload = {"raw": result}
        if not isinstance(payload, dict):
            payload = {"raw": payload}
        payload.setdefault("source", source)
        payload.setdefault("source_id", source_id if isinstance(source_id, str) else "")
        results.append((col_name, payload))
    return results

def run_pipeline(limit: Optional[int] = None, skip: int = 0, only: Optional[List[str]] = None, skip_sources: Optional[Set[str]] = None) -> List[Dict]:
    src_col = get_collection("manga_raw_data", "mangadex_manga")
    projection = {"_id": 1, "id": 1, "attributes.title": 1, "attributes.links": 1}
    client = src_col.database.client
    
    # --- CHANGE START: Gom táº¥t cáº£ cÃ¡c tÃ¡c vá»¥ vÃ o má»™t danh sÃ¡ch duy nháº¥t ---

    # 1. QuÃ©t vÃ  thu tháº­p táº¥t cáº£ cÃ¡c tÃ¡c vá»¥ nhÆ° cÅ©
    all_tasks = []
    logger.info("Scanning source collection for source ids (limit=%s skip=%s)", limit, skip)
    with client.start_session() as session:
        cursor = src_col.find({}, projection, no_cursor_timeout=True, session=session).skip(skip)
        if limit is not None:
            cursor = cursor.limit(limit)
        try:
            for doc in cursor:
                mg_id = str(doc.get("_id"))
                for source in ["anilist", "mal", "mangaupdates", "animeplanet"]:
                    if only and source not in only:
                        continue
                    if skip_sources and source in skip_sources:
                        continue
                    sid = _resolve_source_id_from_doc(doc, source)
                    if not sid:
                        continue
                    all_tasks.append((source, mg_id, sid))
        finally:
            cursor.close()

    # 2. Xá»­ lÃ½ logic batch Ä‘áº·c biá»‡t cho AniList
    final_tasks = []
    anilist_tasks_to_batch = []
    for task in all_tasks:
        if task[0] == 'anilist':
            anilist_tasks_to_batch.append(task)
        else:
            final_tasks.append(task)
    
    # NhÃ³m AniList IDs thÃ nh cÃ¡c batch, má»—i batch lÃ  má»™t tÃ¡c vá»¥
    if anilist_tasks_to_batch:
        for i in range(0, len(anilist_tasks_to_batch), 10):
            batch = anilist_tasks_to_batch[i:i+10]
            batch_ids = [sid for _, _, sid in batch]
            if batch_ids:
                # Láº¥y mangadex_id tá»« tÃ¡c vá»¥ Ä‘áº§u tiÃªn cá»§a batch Ä‘á»ƒ lÃ m Ä‘áº¡i diá»‡n
                representative_mg_id = batch[0][1]
                final_tasks.append(("anilist", representative_mg_id, batch_ids))

    logger.info("Scheduling %d final collector tasks (%d anilist batches)", len(final_tasks), len(anilist_tasks_to_batch) // 10 + 1)

    COLLECTORS = {
        "anilist": collect_anilist,
        "mal": collect_mal,
        "mangaupdates": collect_mangaupdates,
        "animeplanet": collect_animeplanet,
    }

    results = []
    
    # 3. Táº¡o má»™t ThreadPoolExecutor duy nháº¥t vÃ  submit táº¥t cáº£ tÃ¡c vá»¥
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor: # TÄƒng max_workers Ä‘á»ƒ cháº¡y nhiá»u nguá»“n
        # Sá»­ dá»¥ng má»™t dictionary Ä‘á»ƒ map future vá»›i thÃ´ng tin tÃ¡c vá»¥ gá»‘c
        future_to_task = {
            executor.submit(_try_call_collector, COLLECTORS[source], source, mg_id, sid): (source, sid)
            for source, mg_id, sid in final_tasks
        }

        for future in concurrent.futures.as_completed(future_to_task):
            source, original_sid = future_to_task[future]
            try:
                result = future.result()
                if result is None:
                    continue
                
                normalized_results = _normalize_collector_result(source, result, original_sid)
                
                for col_name, payload in normalized_results:
                    # Äáº£m báº£o _id tá»“n táº¡i trong payload trÆ°á»›c khi lÆ°u
                    if "_id" not in payload:
                        effective_sid = payload.get("source_id")
                        if not effective_sid:
                            logger.error("Payload missing '_id' and 'source_id', cannot save. Payload: %s", payload)
                            continue
                        
                        source_prefix = "ap" if source == "animeplanet" else source[:2]
                        payload["_id"] = f"{source_prefix}_{effective_sid}"

                    tgt_col = get_collection("manga_raw_data", col_name)
                    tgt_col.replace_one({"_id": payload["_id"]}, payload, upsert=True)
                    logger.info("[SAVED] %s %s | reviews=%d recs=%d status=%s",
                                col_name, payload["_id"], len(payload.get("reviews", [])), len(payload.get("recommendations", [])), payload.get("status"))
                    results.append(payload)
            except Exception as e:
                logger.error("Collector for source '%s' with id '%s' failed: %s", source, original_sid, e, exc_info=True)

    # --- CHANGE END ---
    
    print("\n=== Fetch Summary ===")
    # Sáº¯p xáº¿p káº¿t quáº£ theo nguá»“n Ä‘á»ƒ dá»… Ä‘á»c
    for payload in sorted(results, key=lambda x: x.get("source", "")):
        print(f"[{payload.get('source')}_data] {payload.get('source_id')} | reviews={len(payload.get('reviews', []))}, recommendations={len(payload.get('recommendations', []))}")
    
    return results
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\pipeline_conservative.py ---
import logging
import os
from typing import Any, Dict, List, Set
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from pymongo import MongoClient
from pymongo.collection import Collection
import scrapy
from scrapy.crawler import CrawlerProcess

from src.extractors import mal
from src.extractors.mal_fetcher import get_full_data

logger = logging.getLogger(__name__)

class Pipeline:
    def __init__(self, limit: int, skip: int, only: Set[str], skip_sources: Set[str]):
        self.limit = limit
        self.skip = skip
        self.only = only
        self.skip_sources = skip_sources
        self.client = MongoClient("mongodb://localhost:27017/")
        self.db = self.client["manga_db"]
        self.source_collection = self.db["source_ids"]
        self.dest_collection = self.db["mal_data"]

    def _get_source_ids(self) -> List[str]:
        """Get source ids to process from source collection."""
        query = {}
        if self.only:
            query["source"] = {"$in": list(self.only)}
        if self.skip_sources:
            query["source"] = {"$nin": list(self.skip_sources)}
        
        cursor = self.source_collection.find(query).skip(self.skip).limit(self.limit)
        source_ids = [doc["source_id"] for doc in cursor]
        logger.info(f"Found {len(source_ids)} source IDs")
        return source_ids

    def _process_mal_id(self, mal_id: str) -> Dict[str, Any]:
        """Process a single MAL ID."""
        try:
            # Use Scrapy for MAL
            process = CrawlerProcess({
                'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'DOWNLOAD_DELAY': 2,
                'CONCURRENT_REQUESTS': 4,
                'LOG_LEVEL': 'INFO',
                'ITEM_PIPELINES': {'__main__.MongoPipeline': 300}
            })
            process.crawl('mal_spider', mal_id=mal_id)
            process.start()
            # Payload will be saved by MongoPipeline
            return {"_id": f"mal_{mal_id}", "status": "ok"}
        except Exception as e:
            logger.error(f"Error processing MAL ID {mal_id}: {e}")
            return mal.get_full_data(mal_id)  # Fallback to requests

    def run(self):
        """Execute pipeline."""
        logger.info("Scanning source collection for source ids")
        source_ids = self._get_source_ids()
        
        mal_ids = [sid for sid in source_ids if sid.startswith("mal_")]
        mal_ids = [sid.replace("mal_", "") for sid in mal_ids]
        
        logger.info(f"Scheduling {len(mal_ids)} final collector tasks")
        
        results = []
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(self._process_mal_id, mal_id) for mal_id in mal_ids]
            for future in futures:
                result = future.result()
                if result:
                    results.append(result)
                    logger.info(f"[SAVED] mal_data {result['_id']} | reviews={len(result.get('reviews', []))} recs={len(result.get('recommendations', []))} status={result.get('status', 'unknown')}")
        
        logger.info(f"\n=== Fetch Summary ===\n")
        for result in results:
            logger.info(f"[mal_data] {result['_id'].replace('mal_', '')} | reviews={len(result.get('reviews', []))}, recommendations={len(result.get('recommendations', []))}")
        logger.info(f"\nğŸ‰ Total results: {len(results)}")

class MongoPipeline:
    def __init__(self):
        self.client = MongoClient("mongodb://localhost:27017/")
        self.db = self.client["manga_db"]
        self.collection = self.db["mal_data"]

    def process_item(self, item, spider):
        self.collection.update_one(
            {"_id": item["_id"]},
            {"$set": item},
            upsert=True
        )
        logger.info(f"[SAVED] mal_data {item['_id']} | reviews={len(item.get('reviews', []))} recs={len(item.get('recommendations', []))} status={item.get('status', 'unknown')}")
        return item

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler("manga_fetch_conservative.log"),
            logging.StreamHandler()
        ]
    )
    pipeline = Pipeline(limit=25, skip=0, only={"mal"}, skip_sources=set())
    pipeline.run()
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\run.py ---
# src/run.py
"""
Runner script for manga data fetch pipeline.
Usage: python -m src.run --limit 5 --only anilist mal
"""

import argparse
import logging
import sys

from .pipeline import run_pipeline

def main():
    parser = argparse.ArgumentParser(
        description="Manga data fetch pipeline from mangadex_manga links"
    )
    parser.add_argument("--limit", type=int, default=None,
                        help="Number of documents to process (default: None - all docs)")
    parser.add_argument("--skip", type=int, default=0,
                        help="Number of documents to skip")
    parser.add_argument("--only", nargs="*",
                        choices=["mal", "anilist", "mangaupdates", "animeplanet"],
                        help="Only run for specific sources")
    parser.add_argument("--skip-animeplanet", action="store_true",
                        help="Skip Anime-Planet fetches temporarily")
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="Enable verbose logging")

    args = parser.parse_args()

    # Setup logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('manga_fetch.log')
        ]
    )

    logger = logging.getLogger(__name__)

    only_sources = args.only

    skip_sources = set()
    if args.skip_animeplanet:
        skip_sources.add("animeplanet")

    logger.info(f"ğŸš€ Starting pipeline")
    logger.info(f"ğŸ“‹ Settings: limit={args.limit}, skip={args.skip}, only={only_sources}, skip_sources={skip_sources}")

    try:
        results = run_pipeline(
            limit=args.limit,
            skip=args.skip,
            only=only_sources,
            skip_sources=skip_sources
        )

        print(f"\nğŸ‰ Total results: {len(results)}")
    except KeyboardInterrupt:
        logger.info("âŒ Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Pipeline failed: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\scrapy_runner.py ---
# src/scrapy_runner.py
"""
Optional helper: run a scrapy runspider script as a subprocess.
Usage:
    from src.scrapy_runner import run_scrapy_runspider
    run_scrapy_runspider("spiders/animeplanet_spider.py", ["-a", "slug=tower-of-god"])
"""

import subprocess
import shlex
import os
from typing import List, Tuple

def run_scrapy_runspider(script_path: str, extra_args: List[str] = None, env: dict = None) -> Tuple[int, str, str]:
    """
    Run: scrapy runspider <script_path> <extra_args...>
    Returns (returncode, stdout, stderr)
    """
    if extra_args is None:
        extra_args = []
    cmd = ["scrapy", "runspider", script_path] + extra_args
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env={**os.environ, **(env or {})})
    out, err = proc.communicate()
    return proc.returncode, out.decode("utf-8", errors="replace"), err.decode("utf-8", errors="replace")
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\utils.py ---
import re
import time
import uuid
from datetime import datetime, timezone

def utc_now_iso():
    return datetime.now(timezone.utc).isoformat()

def is_digits(s: str) -> bool:
    return bool(re.fullmatch(r"\d+", str(s).strip()))

def new_trace_id():
    return uuid.uuid4().hex

def ms(start_ts):
    return int((time.time() - start_ts) * 1000)
=== Folder: src\common ===

--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\common\anti_blocking.py ---
# src/common/anti_blocking.py
"""
Advanced anti-blocking utilities for web scraping.
Provides rotating proxies, user agents, intelligent delays, and session management.
"""

import os
import time
import random
import logging
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

logger = logging.getLogger(__name__)

# Extended user agent pool with real browser fingerprints
USER_AGENTS = [
    # Chrome Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    
    # Chrome Mac
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    
    # Firefox Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:119.0) Gecko/20100101 Firefox/119.0",
    
    # Safari Mac
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
    
    # Edge Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    
    # Mobile Chrome
    "Mozilla/5.0 (Linux; Android 13; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1",
]

# Free proxy sources (you can extend this with paid proxy services)
FREE_PROXY_APIS = [
    "https://api.proxyscrape.com/v2/?request=get&protocol=http&timeout=10000&country=all&ssl=all&anonymity=all",
    "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
]

class ProxyRotator:
    """Manages proxy rotation with health checking."""
    
    def __init__(self):
        self.proxies: List[str] = []
        self.working_proxies: List[str] = []
        self.failed_proxies: set = set()
        self.last_refresh = None
        self.refresh_interval = timedelta(hours=1)
        
    def refresh_proxies(self):
        """Fetch fresh proxy list from free sources."""
        if self.last_refresh and datetime.now() - self.last_refresh < self.refresh_interval:
            return
            
        logger.info("Refreshing proxy list...")
        new_proxies = []
        
        for api_url in FREE_PROXY_APIS:
            try:
                resp = requests.get(api_url, timeout=10)
                if resp.status_code == 200:
                    proxies = resp.text.strip().split('\n')
                    new_proxies.extend([p.strip() for p in proxies if ':' in p])
            except Exception as e:
                logger.warning(f"Failed to fetch proxies from {api_url}: {e}")
                
        # Remove duplicates and failed proxies
        self.proxies = list(set(new_proxies) - self.failed_proxies)
        self.working_proxies = self.proxies.copy()
        self.last_refresh = datetime.now()
        logger.info(f"Loaded {len(self.proxies)} proxies")
        
    def get_proxy(self) -> Optional[Dict[str, str]]:
        """Get a working proxy, refresh list if needed."""
        if not self.working_proxies:
            self.refresh_proxies()
            
        if not self.working_proxies:
            return None
            
        proxy_addr = random.choice(self.working_proxies)
        return {
            "http": f"http://{proxy_addr}",
            "https": f"http://{proxy_addr}"
        }
        
    def mark_proxy_failed(self, proxy_dict: Dict[str, str]):
        """Mark a proxy as failed."""
        if proxy_dict and "http" in proxy_dict:
            proxy_addr = proxy_dict["http"].replace("http://", "")
            self.failed_proxies.add(proxy_addr)
            if proxy_addr in self.working_proxies:
                self.working_proxies.remove(proxy_addr)

class RequestManager:
    """Manages intelligent request timing and session handling."""
    
    def __init__(self, base_delay: float = 5.0, max_delay: float = 60.0):
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.last_request_time = 0
        self.consecutive_failures = 0
        self.session = self._create_session()
        self.proxy_rotator = ProxyRotator()
        
    def _create_session(self) -> requests.Session:
        """Create session with retry strategy."""
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session
        
    def _calculate_delay(self) -> float:
        """Calculate intelligent delay based on recent failures."""
        base = self.base_delay
        
        # Exponential backoff for consecutive failures
        if self.consecutive_failures > 0:
            base *= (2 ** min(self.consecutive_failures, 5))
            
        # Add random jitter (Â±25%)
        jitter = random.uniform(0.75, 1.25)
        delay = min(base * jitter, self.max_delay)
        
        # Add extra random delay (0-10 seconds)
        delay += random.uniform(0, 10)
        
        return delay
        
    def _get_headers(self) -> Dict[str, str]:
        """Generate realistic headers with random user agent."""
        ua = random.choice(USER_AGENTS)
        
        # Common browser headers that match the user agent
        headers = {
            "User-Agent": ua,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9,vi;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0",
        }
        
        # Randomly add some optional headers
        if random.random() < 0.5:
            headers["Referer"] = random.choice([
                "https://www.google.com/",
                "https://www.bing.com/",
                "https://duckduckgo.com/",
                "https://www.anime-planet.com/"
            ])
            
        return headers
        
    def make_request(self, url: str, use_proxy: bool = True, max_retries: int = 3) -> Optional[requests.Response]:
        """Make a request with intelligent timing and anti-blocking measures."""
        
        # Wait for appropriate delay
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        delay = self._calculate_delay()
        
        if time_since_last < delay:
            sleep_time = delay - time_since_last
            logger.info(f"Waiting {sleep_time:.1f}s before next request...")
            time.sleep(sleep_time)
            
        self.last_request_time = time.time()
        
        for attempt in range(max_retries):
            try:
                headers = self._get_headers()
                proxies = None
                
                if use_proxy and random.random() < 0.7:  # Use proxy 70% of the time
                    proxies = self.proxy_rotator.get_proxy()
                    if proxies:
                        logger.debug(f"Using proxy: {proxies['http']}")
                
                resp = self.session.get(
                    url, 
                    headers=headers, 
                    proxies=proxies,
                    timeout=30,
                    allow_redirects=True
                )
                
                if resp.status_code == 200:
                    self.consecutive_failures = 0
                    logger.info(f"âœ“ Success: {url} (status: {resp.status_code})")
                    return resp
                    
                elif resp.status_code == 403:
                    logger.warning(f"âœ— Blocked (403): {url} - attempt {attempt + 1}")
                    if proxies:
                        self.proxy_rotator.mark_proxy_failed(proxies)
                    self.consecutive_failures += 1
                    
                    # Longer delay after 403
                    if attempt < max_retries - 1:
                        backoff_delay = (2 ** attempt) * 10 + random.uniform(5, 15)
                        logger.info(f"Backing off for {backoff_delay:.1f}s after 403...")
                        time.sleep(backoff_delay)
                        
                elif resp.status_code == 429:
                    logger.warning(f"âœ— Rate limited (429): {url}")
                    self.consecutive_failures += 1
                    
                    # Extract retry-after header if present
                    retry_after = resp.headers.get('Retry-After')
                    if retry_after:
                        try:
                            delay = int(retry_after) + random.uniform(5, 15)
                        except ValueError:
                            delay = 60 + random.uniform(10, 30)
                    else:
                        delay = 60 + random.uniform(10, 30)
                        
                    if attempt < max_retries - 1:
                        logger.info(f"Rate limited, waiting {delay:.1f}s...")
                        time.sleep(delay)
                        
                else:
                    logger.warning(f"âœ— HTTP {resp.status_code}: {url}")
                    self.consecutive_failures += 1
                    
            except requests.exceptions.ProxyError as e:
                logger.warning(f"Proxy error: {e}")
                if proxies:
                    self.proxy_rotator.mark_proxy_failed(proxies)
                    
            except requests.exceptions.Timeout as e:
                logger.warning(f"Timeout: {e}")
                
            except Exception as e:
                logger.error(f"Request error: {e}")
                
            # Wait before retry
            if attempt < max_retries - 1:
                retry_delay = (2 ** attempt) * 5 + random.uniform(2, 8)
                time.sleep(retry_delay)
                
        self.consecutive_failures += 1
        logger.error(f"âœ— Failed all {max_retries} attempts for {url}")
        return None

# Global instance
request_manager = RequestManager()

def get_request_manager() -> RequestManager:
    """Get the global request manager instance."""
    return request_manager

def smart_delay(min_delay: float = 5.0, max_delay: float = 30.0):
    """Add intelligent random delay between requests."""
    delay = random.uniform(min_delay, max_delay)
    
    # Add extra delay during peak hours (assuming UTC)
    current_hour = datetime.utcnow().hour
    if 8 <= current_hour <= 22:  # Peak hours
        delay *= random.uniform(1.2, 1.8)
        
    logger.debug(f"Smart delay: {delay:.1f}s")
    time.sleep(delay)

def get_random_headers() -> Dict[str, str]:
    """Generate realistic browser headers."""
    ua = random.choice(USER_AGENTS)
    
    headers = {
        "User-Agent": ua,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": random.choice([
            "en-US,en;q=0.9",
            "en-US,en;q=0.9,vi;q=0.8",
            "en-GB,en;q=0.9",
        ]),
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
    }
    
    # Randomly add referer
    if random.random() < 0.6:
        headers["Referer"] = random.choice([
            "https://www.google.com/",
            "https://www.bing.com/",
            "https://duckduckgo.com/",
            "https://www.anime-planet.com/",
            "https://myanimelist.net/",
        ])
        
    return headers

--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\common\config.py ---
import os
from dotenv import load_dotenv

load_dotenv()

# Database settings
MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017/")
MONGO_DB = os.getenv("MONGO_DB", "manga_raw_data")
MONGODX_COLLECTION = os.getenv("MONGODX_COLLECTION", "mangadx_manga")

# API endpoints
ANI_API_ENDPOINT = os.getenv("ANI_API_ENDPOINT", "https://graphql.anilist.co")
MAL_CLIENT_ID = os.getenv("MAL_CLIENT_ID", "6114d00ca681b7701d1e15fe11a4987e")  # Default public client ID
MU_API_BASE = os.getenv("MU_API_BASE", "https://api.mangaupdates.com/v1")

# Proxy settings: Load from proxy_ip.txt
PROXY_FILE = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../proxy_ip.txt"))
HTTP_PROXY = []

try:
    if os.path.exists(PROXY_FILE):
        with open(PROXY_FILE, "r", encoding="utf-8") as f:
            for line in f:
                proxy = line.strip()
                if proxy:
                    # Ensure proxy has http:// prefix
                    if not proxy.startswith(("http://", "https://")):
                        proxy = f"http://{proxy}"
                    HTTP_PROXY.append(proxy)
    else:
        print(f"Warning: Proxy file {PROXY_FILE} not found, no proxies will be used.")
except Exception as e:
    print(f"Error reading proxy file {PROXY_FILE}: {e}")

# Fallback proxies if file is empty or fails
if not HTTP_PROXY:
    HTTP_PROXY = [
        "http://45.79.139.169:80",
        "http://104.236.195.251:80",
        "http://159.65.0.132:80",
    ]

HTTPS_PROXY = os.getenv("HTTPS_PROXY", None)

# Data lake path
DATA_LAKE_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../data-lake/raw"))

# Anti-blocking configuration
ANTI_BLOCKING_CONFIG = {
    # Base delays (seconds)
    "MIN_DELAY": float(os.getenv("MIN_DELAY", "10")),
    "MAX_DELAY": float(os.getenv("MAX_DELAY", "30")),
    "REVIEW_DELAY_MIN": float(os.getenv("REVIEW_DELAY_MIN", "8")),
    "REVIEW_DELAY_MAX": float(os.getenv("REVIEW_DELAY_MAX", "20")),
    
    # Retry configuration
    "MAX_RETRIES": int(os.getenv("MAX_RETRIES", "5")),
    "BACKOFF_MULTIPLIER": float(os.getenv("BACKOFF_MULTIPLIER", "2.0")),
    "BACKOFF_MAX": float(os.getenv("BACKOFF_MAX", "300")),  # 5 minutes max
    
    # Request timeout
    "REQUEST_TIMEOUT": float(os.getenv("REQUEST_TIMEOUT", "30")),
    
    # Use proxies (enabled since we have proxy_ip.txt)
    "USE_PROXIES": os.getenv("USE_PROXIES", "true").lower() == "true",
    
    # Conservative mode (longer delays, fewer retries)
    "CONSERVATIVE_MODE": os.getenv("CONSERVATIVE_MODE", "true").lower() == "true",
}
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\common\db.py ---
from pymongo import MongoClient
from .config import MONGO_URI, MONGO_DB, MONGODEX_COLLECTION

def get_db():
    client = MongoClient(MONGO_URI)
    return client[MONGO_DB]

def iter_mangadex_docs(limit=None):
    db = get_db()
    proj = {
        "_id": 1,
        "id": 1,
        "attributes.title": 1,
        "attributes.links": 1
    }
    cur = db[MONGODEX_COLLECTION].find({"attributes.links": {"$exists": True}}, proj)
    if limit:
        cur = cur.limit(int(limit))
    for doc in cur:
        yield doc
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\common\http.py ---
import aiohttp, asyncio
from aiolimiter import AsyncLimiter
from tenacity import retry, stop_after_attempt, wait_exponential

class HttpClient:
    def __init__(self, rate_per_sec=2, headers=None, proxy=None):
        self.limiter = AsyncLimiter(rate_per_sec, 1)
        self.session = None
        self.headers = headers or {}
        self.proxy = proxy

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(headers=self.headers, trust_env=True)
        return self

    async def __aexit__(self, *args):
        await self.session.close()

    @retry(stop=stop_after_attempt(5), wait=wait_exponential(min=1, max=30))
    async def get_json(self, url, **kw):
        async with self.limiter:
            async with self.session.get(url, proxy=self.proxy, **kw) as r:
                txt = await r.text()
                return r.status, txt, (await r.json(content_type=None))

    @retry(stop=stop_after_attempt(5), wait=wait_exponential(min=1, max=30))
    async def post_json(self, url, json=None, **kw):
        async with self.limiter:
            async with self.session.post(url, json=json, proxy=self.proxy, **kw) as r:
                txt = await r.text()
                return r.status, txt, (await r.json(content_type=None))
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\common\io.py ---
import os, gzip, json, datetime, hashlib

from .config import DATA_LAKE_ROOT

def _date_parts(dt=None):
    if not dt:
        dt = datetime.datetime.utcnow()
    return dt.strftime("%Y"), dt.strftime("%m"), dt.strftime("%d"), dt

def _safe_name(key: str):
    digest = hashlib.sha1(key.encode("utf-8")).hexdigest()
    return digest

def write_jsonl(source: str, key: str, envelope: dict):
    y, m, d, now = _date_parts()
    out_dir = os.path.join(DATA_LAKE_ROOT, source, f"YYYY={y}", f"MM={m}", f"DD={d}")
    os.makedirs(out_dir, exist_ok=True)
    fname = f"{_safe_name(key)}.jsonl.gz"
    path = os.path.join(out_dir, fname)
    line = json.dumps(envelope, ensure_ascii=False) + "\n"
    with gzip.open(path, "at", encoding="utf-8") as f:
        f.write(line)
    return path
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\common\util.py ---

=== Folder: src\extractors ===

--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\extractors\anilist.py ---
import logging
from .anilist_fetcher import get_full_data, get_full_data_parallel
from typing import List

logger = logging.getLogger(__name__)

def collect_anilist(mangadx_id: str, source_id: str):
    """
    Collector entrypoint for AniList.
    - source_id: AniList numeric id
    """
    try:
        payload = get_full_data(source_id)
        payload.setdefault("_id", f"anilist_{source_id}")
        payload.setdefault("source", "anilist")
        payload.setdefault("source_id", source_id)
        return payload
    except Exception as e:
        logger.error("collect_anilist failed for %s: %s", source_id, e, exc_info=True)
        return {
            "_id": f"anilist_{source_id}",
            "source": "anilist",
            "source_id": source_id,
            "reviews": [],
            "recommendations": [],
            "status": "error",
            "http": {"error": str(e)},
        }

def collect_anilist_batch(manga_ids: List[str], use_parallel: bool = True):
    """
    High-performance batch collector for 87k objects in 24h
    - manga_ids: List of AniList IDs
    - use_parallel: Enable parallel processing (3 workers)
    """
    try:
        if use_parallel and len(manga_ids) > 50:
            return get_full_data_parallel(manga_ids, max_workers=3)
        else:
            return get_full_data(manga_ids)
    except Exception as e:
        logger.error("collect_anilist_batch failed: %s", e, exc_info=True)
        return []
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\extractors\anilist_fetcher.py ---
import logging
from datetime import datetime
from typing import Dict, List
import requests
import random
import time
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
import requests.exceptions
from ..common.config import ANTI_BLOCKING_CONFIG

logger = logging.getLogger(__name__)

ANILIST_API = "https://graphql.anilist.co"

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]

# Configuration cho 87k objects trong 24h
TARGET_OBJECTS_PER_HOUR = 87000 / 24  # ~3625 objects/hour
TARGET_REQUESTS_PER_HOUR = TARGET_OBJECTS_PER_HOUR / 10  # ~362 requests/hour vá»›i batch size 10
SECONDS_PER_REQUEST = 3600 / TARGET_REQUESTS_PER_HOUR  # ~10 giÃ¢y/request

@retry(
    retry=retry_if_exception_type((requests.exceptions.HTTPError, requests.exceptions.RequestException)),
    wait=wait_exponential(multiplier=1.2, min=5, max=60),  # Aggressive: min 5s, max 1 phÃºt
    stop=stop_after_attempt(3),
    reraise=True
)
def _query_anilist_batch(manga_ids: List[str]) -> List[Dict]:
    """Query AniList API with aggressive rate limiting to avoid 429 errors"""
    query = """
    query ($ids: [Int]) {
      Page {
        media(id_in: $ids, type: MANGA) {
          id
          title { romaji english native }
          recommendations { edges { node { mediaRecommendation { id title { romaji } } } } }
          reviews { nodes { summary body } }
        }
      }
    }
    """
    variables = {"ids": [int(id) for id in manga_ids]}
    headers = {
        "User-Agent": random.choice(USER_AGENTS),
        "Content-Type": "application/json",
        "Accept": "application/json",
    }
    # Minimal delay cho 24h target - chá»‰ 2-5s
    delay = random.uniform(2, 5)
    logger.info(f"Pre-request delay: {delay:.1f}s")
    time.sleep(delay)
    
    try:
        r = requests.post(ANILIST_API, json={"query": query, "variables": variables}, headers=headers, timeout=30)
        
        # Kiá»ƒm tra rate limit headers
        remaining = int(r.headers.get("X-RateLimit-Remaining", 90))
        reset_time = int(r.headers.get("X-RateLimit-Reset", 60))
        
        logger.info(f"Rate limit remaining: {remaining}, reset in: {reset_time}s")
        
        # Minimal rate limiting cho 24h target
        if remaining < 5:  # Chá»‰ sleep khi ráº¥t Ã­t quota
            delay = random.uniform(10, 20)  # Chá»‰ 10-20s
            logger.warning(f"Very low rate limit ({remaining}), sleeping {delay:.1f}s")
            time.sleep(delay)
        elif remaining < 15:
            time.sleep(random.uniform(2, 5))  # Delay ráº¥t nháº¹
        
        r.raise_for_status()
        
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 429:
            # 429 handling cho 24h target
            retry_after = int(e.response.headers.get("Retry-After", 60))  # Default 1 phÃºt
            buffer = random.uniform(5, 15)  # Minimal buffer
            total_wait = retry_after + buffer
            logger.error(f"Rate limited! Waiting {total_wait:.1f}s before retry")
            time.sleep(total_wait)
        raise
    return r.json()["data"]["Page"]["media"]

def get_full_data_parallel(al_id: str | List[str], max_workers: int = 3) -> List[Dict]:
    """Parallel version for high-volume processing"""
    if isinstance(al_id, str):
        al_id = [al_id]
    
    # Chia thÃ nh chunks cho parallel processing
    chunk_size = len(al_id) // max_workers if len(al_id) > max_workers else len(al_id)
    chunks = [al_id[i:i+chunk_size] for i in range(0, len(al_id), chunk_size)]
    
    all_payloads = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(get_full_data, chunk) for chunk in chunks]
        for future in futures:
            all_payloads.extend(future.result())
    
    return all_payloads

def get_full_data(al_id: str | List[str]) -> List[Dict]:
    if isinstance(al_id, str):
        al_id = [al_id]  # Convert single ID to list for consistency
    
    payloads = []
    for i in range(0, len(al_id), 10):  # TÄƒng batch size lÃªn 10 cho 24h target
        batch_ids = al_id[i:i+10]
        
        # Minimal batch delay cho 24h target
        if i > 0:
            batch_delay = random.uniform(5, 8)  # Chá»‰ 5-8s giá»¯a cÃ¡c batch
            logger.info(f"Waiting {batch_delay:.1f}s before next batch...")
            time.sleep(batch_delay)
        try:
            media_list = _query_anilist_batch(batch_ids)
            for media in media_list:
                al_id_str = str(media["id"])
                payload = {
                    "_id": f"anilist_{al_id_str}",
                    "source": "anilist",
                    "source_id": al_id_str,
                    "source_url": f"https://anilist.co/manga/{al_id_str}",
                    "fetched_at": datetime.utcnow().isoformat(),
                }
                recs = [{"id": str(edge["node"]["mediaRecommendation"]["id"]),
                        "title": edge["node"]["mediaRecommendation"]["title"]["romaji"]}
                       for edge in media["recommendations"]["edges"] if edge["node"]["mediaRecommendation"]]
                reviews = [{"text": r.get("summary") or r.get("body")}
                          for r in media["reviews"]["nodes"] if (r.get("summary") or r.get("body"))]
                payload["recommendations"] = recs
                payload["reviews"] = reviews
                payload["status"] = "ok" if (reviews or recs) else "no_reviews"
                payload["http"] = {"code": 200}
                payloads.append(payload)
        except Exception as e:
            logger.error("AniList batch fetch failed for IDs %s: %s", batch_ids, e, exc_info=True)
            for bid in batch_ids:
                payloads.append({
                    "_id": f"anilist_{bid}",
                    "source": "anilist",
                    "source_id": bid,
                    "source_url": f"https://anilist.co/manga/{bid}",
                    "fetched_at": datetime.utcnow().isoformat(),
                    "recommendations": [],
                    "reviews": [],
                    "status": "error",
                    "http": {"error": str(e)}
                })
    
    return payloads
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\extractors\animeplanet.py ---
import logging
from .animeplanet_fetcher import get_full_data

logger = logging.getLogger(__name__)

def collect_animeplanet(mangadex_id: str, source_id: str):
    """
    Collector entrypoint for Anime-Planet.
    - mangadex_id: MangaDex document id
    - source_id: slug of anime-planet manga (e.g. "naruto")
    """
    try:
        payload = get_full_data(source_id)
        payload.setdefault("_id", f"ap_{source_id}")
        payload.setdefault("source", "animeplanet")
        payload.setdefault("source_id", source_id)
        return payload
    except Exception as e:
        logger.error("collect_animeplanet failed for %s: %s", source_id, e, exc_info=True)
        return {
            "_id": f"ap_{source_id}",
            "source": "animeplanet",
            "source_id": source_id,
            "reviews": [],
            "recommendations": [],
            "status": "error",
            "http": {"error": str(e)},
        }
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\extractors\animeplanet_fetcher.py ---
import asyncio
import logging
import subprocess
import sys
from datetime import datetime
from typing import Dict, List, Optional

from bs4 import BeautifulSoup

from src.scrapy_runner import run_scrapy_runspider
from src.db import get_collection

logger = logging.getLogger(__name__)

ANIMEPLANET_BASE = "https://www.anime-planet.com"


# ---------------- Playwright Setup ----------------
def _ensure_playwright_browsers_installed() -> bool:
    try:
        subprocess.run([sys.executable, "-m", "playwright", "--version"],
                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False)
        subprocess.run([sys.executable, "-m", "playwright", "install", "chromium"],
                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False)
        return True
    except Exception as e:
        logger.warning("Playwright install failed: %s", e)
        return False


async def _fetch_with_playwright(url: str, timeout_ms: int = 45000) -> Optional[str]:
    try:
        from playwright.async_api import async_playwright
    except ImportError:
        logger.error("Playwright not installed")
        return None

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=["--no-sandbox"])
            context = await browser.new_context(
                user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                            "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"),
                locale="en-US"
            )
            page = await context.new_page()
            await page.goto(url, timeout=timeout_ms)
            await page.wait_for_selector("body", timeout=8000)
            html = await page.content()
            await browser.close()
            return html
    except Exception as e:
        logger.error("Playwright fetch failed for %s: %s", url, e)
        return None


def _run_scrapy_and_read(slug: str) -> Optional[Dict]:
    rc, out, err = run_scrapy_runspider("spiders/animeplanet_spider.py", ["-a", f"slug={slug}"])
    logger.info("Scrapy fallback rc=%s", rc)
    try:
        col = get_collection("manga_raw_data", "animeplanet_data")
        return col.find_one({"_id": f"ap_{slug}"})
    except Exception as e:
        logger.error("Scrapy fallback read failed: %s", e)
        return None


# ---------------- Parsers ----------------
def _parse_main(html: str) -> Dict:
    soup = BeautifulSoup(html, "lxml")
    title = soup.select_one("h1")
    title = title.get_text(strip=True) if title else None

    synopsis = ""
    s_node = soup.select_one("div.synopsis p")
    if s_node:
        synopsis = s_node.get_text(" ", strip=True)

    rating = None
    rc = soup.select_one("div.avgRating, span[itemprop='ratingValue']")
    if rc:
        rating = rc.get_text(strip=True)

    img = None
    og_img = soup.select_one("meta[property='og:image']")
    if og_img:
        img = og_img.get("content")

    authors = [a.get_text(strip=True) for a in soup.select("a[href*='/people/']")]
    genres = [g.get_text(strip=True) for g in soup.select("a[href*='/manga/genres/']")]

    return {
        "title": title,
        "synopsis": synopsis,
        "rating": rating,
        "image": img,
        "authors": list(dict.fromkeys(authors)),
        "genres": list(dict.fromkeys(genres)),
    }


def _parse_reviews(html: str) -> List[Dict]:
    soup = BeautifulSoup(html, "lxml")
    reviews = []
    for div in soup.select(".reviewText, .user-review, article.review"):
        text = div.get_text(" ", strip=True)
        if text:
            reviews.append({"text": text})
    return reviews


def _parse_recommendations(html: str) -> List[Dict]:
    soup = BeautifulSoup(html, "lxml")
    recs = []
    for a in soup.select("a[href*='/manga/']"):
        href = a.get("href", "")
        if "/manga/" in href:
            slug = href.split("/manga/")[-1].split("?")[0].strip("/")
            recs.append({"slug": slug, "url": ANIMEPLANET_BASE + href})
    return list({r["slug"]: r for r in recs}.values())


# ---------------- Public API ----------------
def get_full_data(slug: str) -> Dict:
    payload = {
        "_id": f"ap_{slug}",
        "source": "animeplanet",
        "source_id": slug,
        "source_url": f"{ANIMEPLANET_BASE}/manga/{slug}",
        "fetched_at": datetime.utcnow().isoformat(),
    }

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        html_main = loop.run_until_complete(
            _fetch_with_playwright(f"{ANIMEPLANET_BASE}/manga/{slug}")
        )
        html_reviews = loop.run_until_complete(
            _fetch_with_playwright(f"{ANIMEPLANET_BASE}/manga/{slug}/reviews")
        )
        html_recs = loop.run_until_complete(
            _fetch_with_playwright(f"{ANIMEPLANET_BASE}/manga/{slug}/recommendations")
        )
    finally:
        loop.close()

    if html_main:
        payload["main"] = _parse_main(html_main)
    else:
        payload["main"] = {}

    payload["reviews"] = _parse_reviews(html_reviews) if html_reviews else []
    payload["recommendations"] = _parse_recommendations(html_recs) if html_recs else []

    payload["http"] = {"code": 200 if html_main else 500}
    payload["status"] = "ok" if (payload["reviews"] or payload["recommendations"]) else "no_reviews"

    if not html_main and not html_reviews and not html_recs:
        logger.info("Playwright failed, trying Scrapy fallback for %s", slug)
        doc = _run_scrapy_and_read(slug)
        if doc:
            return doc
        payload.update({"http": {"error": "could_not_fetch"}, "status": "error"})

    return payload


def get_reviews(slug: str) -> List[Dict]:
    return get_full_data(slug).get("reviews", [])
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\extractors\animeplanet_fetcher_enhanced.py ---
# src/extractors/animeplanet_fetcher_enhanced.py
import asyncio
import logging
import random
import subprocess
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple

from bs4 import BeautifulSoup

# Try to import cloudscraper if available; if not, we'll fallback to requests and Playwright.
try:
    import cloudscraper  # type: ignore
except Exception:
    cloudscraper = None  # type: ignore

# Local project fallbacks (scrapy spider, mongo read) kept for compatibility
try:
    from src.scrapy_runner import run_scrapy_runspider
    from src.db import get_collection
except Exception:
    # Allow imports to fail in editors that don't have project path; runtime will have them.
    run_scrapy_runspider = None  # type: ignore
    get_collection = None  # type: ignore

logger = logging.getLogger(__name__)

ANIMEPLANET_BASE = "https://www.anime-planet.com"
DEFAULT_HEADERS = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
}


USER_AGENTS = [
    # a small list, rotated per-request
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]


def _make_cloudscraper_session():
    """
    Return a cloudscraper session if available, else None.
    """
    if cloudscraper is None:
        return None
    try:
        # cloudscraper.create_scraper() will try to handle CF anti-bot,
        # but may not handle Turnstile challenges.
        s = cloudscraper.create_scraper(browser={
            "browser": "chrome",
            "platform": "windows",
            "mobile": False
        })
        return s
    except Exception as e:
        logger.debug("Could not create cloudscraper session: %s", e)
        return None


def _is_challenge_html(text: str, status_code: Optional[int] = None) -> bool:
    """
    Heuristics to detect Cloudflare/Turnstile or other challenge pages.
    """
    if status_code == 403:
        return True
    low = (text or "").lower()
    challenge_signs = [
        "verifying you are human",
        "just a moment",
        "cdn-cgi/challenge-platform",
        "__cf_chl_tk",
        "turnstile",
        "cf_chl_",
        "ray id:",
        "challenge-platform",
    ]
    for s in challenge_signs:
        if s in low:
            return True
    return False


def _ensure_playwright_browsers_installed() -> bool:
    """
    Attempt to run: python -m playwright install chromium
    Returns True if the subprocess ran without crashing (idempotent).
    """
    try:
        cmd = [sys.executable, "-m", "playwright", "install", "chromium"]
        subprocess.run(cmd, check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, timeout=120)
        return True
    except Exception as e:
        logger.debug("playwright install attempt failed: %s", e)
        return False


async def _fetch_with_playwright_url(url: str, timeout_ms: int = 45000) -> Optional[str]:
    """
    Async fetch using Playwright. Tries chromium, firefox, webkit.
    Returns HTML or None.
    """
    try:
        from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError  # type: ignore
    except Exception as e:
        logger.debug("playwright not importable: %s", e)
        return None

    browser_order = ["chromium", "firefox", "webkit"]
    # try twice: first normally, if missing executable -> try to install then retry
    for attempt in range(2):
        for engine in browser_order:
            try:
                async with async_playwright() as p:
                    browser_type = getattr(p, engine)
                    try:
                        browser = await browser_type.launch(headless=True, args=["--no-sandbox", "--disable-dev-shm-usage"])
                    except Exception as le:
                        msg = str(le).lower()
                        # if missing executables detected, break to install
                        if ("executable doesn't exist" in msg or "could not find" in msg or "no such file or directory" in msg):
                            logger.warning("Playwright executable missing for %s: %s", engine, le)
                            raise le
                        logger.debug("playwright launch error for %s: %s", engine, le)
                        raise le

                    ctx = await browser.new_context(user_agent=random.choice(USER_AGENTS), locale="en-US")
                    page = await ctx.new_page()
                    logger.debug("Playwright navigating to %s (engine=%s)", url, engine)
                    await page.goto(url, timeout=timeout_ms)
                    # Wait a bit for JS and network idle; tolerant to timeout
                    try:
                        await page.wait_for_load_state("networkidle", timeout=8000)
                    except PlaywrightTimeoutError:
                        pass
                    try:
                        html = await page.content()
                    finally:
                        try:
                            await browser.close()
                        except Exception:
                            pass
                    return html
            except Exception as e:
                logger.debug("playwright engine %s failed on attempt %d: %s", engine, attempt, e)
                # if this looks like missing executable -> attempt install & retry outer loop
                if attempt == 0:
                    # try install once
                    installed = _ensure_playwright_browsers_installed()
                    if installed:
                        logger.info("Attempted playwright install; retrying playwright fetch.")
                        # small pause to allow files to settle
                        await asyncio.sleep(0.5)
                        continue
                await asyncio.sleep(0.1)
                continue
    logger.debug("All playwright attempts failed for url %s", url)
    return None


def _parse_main_and_recommendations(html: str) -> Dict:
    """
    Parse main metadata and recommendations from the overview page HTML.
    Returns dict with keys: title, synopsis, rating, image, authors, genres, recommendations (list of {slug,url})
    """
    soup = BeautifulSoup(html, "lxml")
    # title
    title = None
    h1 = soup.select_one("h1")
    if h1:
        title = h1.get_text(strip=True)
    else:
        meta = soup.select_one("meta[property='og:title'], meta[name='title']")
        if meta:
            title = meta.get("content")
    # synopsis
    synopsis = ""
    s_node = soup.select_one("div.synopsis p")
    if s_node:
        synopsis = s_node.get_text(" ", strip=True)
    else:
        meta_desc = soup.select_one("meta[name='description']")
        if meta_desc:
            synopsis = meta_desc.get("content", "")
    # rating
    rating = None
    rc = soup.select_one("div.avgRating, div.rating, span.score")
    if rc:
        rating = rc.get_text(strip=True)
    else:
        meta_rating = soup.select_one("meta[itemprop='ratingValue']")
        if meta_rating:
            rating = meta_rating.get("content")
    # image
    img = None
    og_img = soup.select_one("meta[property='og:image']")
    if og_img:
        img = og_img.get("content")
    else:
        imgnode = soup.select_one("img.media-object, img.seriesImage")
        if imgnode:
            img = imgnode.get("src")
    # authors
    authors = []
    for a in soup.select("a[href*='/people/'], a[href*='/manga/author']"):
        t = a.get_text(strip=True)
        if t:
            authors.append(t)
    authors = list(dict.fromkeys(authors))
    # genres
    genres = []
    for g in soup.select("a[href*='/genres/'], a[href*='/manga/genre']"):
        t = g.get_text(strip=True)
        if t:
            genres.append(t)
    genres = list(dict.fromkeys(genres))
    # recommendations: look first for dedicated blocks, else scan anchors
    recs = []
    for blk in soup.select("section, div"):
        snippet = " ".join(blk.get_text(" ", strip=True).split()[:30]).lower()
        if any(k in snippet for k in ("recommend", "recommendations", "you might like", "similar")):
            for a in blk.select("a[href*='/manga/']"):
                href = a.get("href", "").strip()
                if not href:
                    continue
                href_full = (ANIMEPLANET_BASE + href) if href.startswith("/") else href
                if "/manga/" in href_full:
                    slug = href_full.split("/manga/")[-1].split("?")[0].split("#")[0].strip("/")
                    if slug:
                        recs.append({"slug": slug, "url": href_full})
            if recs:
                break
    if not recs:
        seen = set()
        out = []
        for a in soup.select("a[href*='/manga/']"):
            href = a.get("href", "").strip()
            if not href:
                continue
            href_full = (ANIMEPLANET_BASE + href) if href.startswith("/") else href
            if "/manga/" in href_full:
                slug = href_full.split("/manga/")[-1].split("?")[0].split("#")[0].strip("/")
                if slug and slug not in seen:
                    seen.add(slug)
                    out.append({"slug": slug, "url": href_full})
            if len(out) >= 50:
                break
        recs = out
    return {
        "title": title,
        "synopsis": synopsis,
        "rating": rating,
        "image": img,
        "authors": authors,
        "genres": genres,
        "recommendations": recs,
    }


def _parse_reviews(html: str) -> List[Dict]:
    """
    Parse reviews from reviews page HTML.
    """
    soup = BeautifulSoup(html, "lxml")
    reviews = []
    for div in soup.select(".reviewText, .user-review, article.review, .review"):
        text = div.get_text(" ", strip=True)
        if text:
            reviews.append({"text": text})
    if not reviews:
        for li in soup.select("li.review, li.comment"):
            text = li.get_text(" ", strip=True)
            if text:
                reviews.append({"text": text})
    return reviews


def _run_scrapy_and_read(slug: str) -> Optional[Dict]:
    """
    Optional fallback: run a scrapy runspider for animeplanet spider (if available)
    and then read the result from Mongo. Works only if project includes spider and Mongo.
    """
    if run_scrapy_runspider is None or get_collection is None:
        return None
    try:
        rc, out, err = run_scrapy_runspider("spiders/animeplanet_spider.py", ["-a", f"slug={slug}"])
        logger.info("Scrapy runspider rc=%s stdout_len=%d stderr_len=%d", rc, len(out or ""), len(err or ""))
        col = get_collection("manga_raw_data", "animeplanet_data")
        doc = col.find_one({"_id": f"ap_{slug}"})
        return doc
    except Exception as e:
        logger.exception("Scrapy fallback failed: %s", e)
        return None


def get_full_data(slug: str, max_retries: int = 3, conservative_wait: bool = False) -> Dict:
    """
    Main synchronous entrypoint used by pipeline.
    Attempts (in order):
      1) cloudscraper session (if available) to fetch overview page
      2) if cloudscraper indicates Cloudflare/Turnstile or 403: fallback to Playwright
      3) if both fail: optional scrapy-runspider fallback
    Returns payload with keys:
      _id, source, source_id, source_url, fetched_at, raw_prefix, main, reviews, recommendations, http, status
    """
    payload = {
        "_id": f"ap_{slug}",
        "source": "animeplanet",
        "source_id": slug,
        "source_url": f"{ANIMEPLANET_BASE}/manga/{slug}",
        "fetched_at": datetime.utcnow().isoformat(),
    }

    session = _make_cloudscraper_session()
    html_main = None
    used_playwright = False

    # 1) Try cloudscraper (or requests if cloudscraper missing)
    if session is not None:
        headers = dict(DEFAULT_HEADERS)
        headers["User-Agent"] = random.choice(USER_AGENTS)
        url = f"{ANIMEPLANET_BASE}/manga/{slug}"
        try:
            for attempt in range(max_retries):
                try:
                    r = session.get(url, headers=headers, timeout=30)
                    status = getattr(r, "status_code", None)
                    text = getattr(r, "text", "")
                    logger.info("Request to %s: status=%s content_length=%d", url, status, len(text or ""))
                    if status == 200 and not _is_challenge_html(text, status):
                        html_main = text
                        break
                    # if challenge or 403 => retry a few times then escalate to playwright
                    if _is_challenge_html(text, status) or status == 403:
                        logger.warning("Detected challenge or 403 for %s (attempt %d)", url, attempt + 1)
                        # backoff: if conservative mode wait longer
                        wait = 10 + attempt * (20 if conservative_wait else 5) + random.random() * 3
                        time.sleep(wait)
                        continue
                    # other codes: wait a bit and retry
                    time.sleep(1.0 + random.random() * 1.5)
                except Exception as e:
                    logger.debug("Cloudscraper request error (attempt %d): %s", attempt + 1, e)
                    time.sleep(1.0 + random.random() * 0.5)
            else:
                logger.info("Cloudscraper attempts exhausted for %s", url)
        except Exception as e:
            logger.debug("Cloudscraper session fetch failed: %s", e)

    # 2) If no html_main from cloudscraper or session missing -> try Playwright
    if not html_main:
        logger.info("Falling back to Playwright for %s", slug)
        used_playwright = True
        try:
            # use a new event loop for synchronous call
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            html_main = loop.run_until_complete(_fetch_with_playwright_url(f"{ANIMEPLANET_BASE}/manga/{slug}"))
        except Exception as e:
            logger.debug("Playwright fetch main failed: %s", e)
            html_main = None
        finally:
            try:
                loop.close()
            except Exception:
                pass

    if html_main:
        payload["raw_prefix"] = html_main[:20000]
        main = _parse_main_and_recommendations(html_main)
        payload["main"] = {k: v for k, v in main.items() if k != "recommendations"}
        # recommendations from main
        payload["recommendations"] = main.get("recommendations", [])
        # 3) Fetch /recommendations endpoint (dedicated) â€” try cloudscraper first, else playwright
        rec_html = None
        rec_url = f"{ANIMEPLANET_BASE}/manga/{slug}/recommendations"
        # cloudscraper attempt
        if session is not None:
            try:
                headers = dict(DEFAULT_HEADERS)
                headers["User-Agent"] = random.choice(USER_AGENTS)
                r = session.get(rec_url, headers=headers, timeout=30)
                status = getattr(r, "status_code", None)
                text = getattr(r, "text", "")
                logger.info("Recommendations request %s status=%s len=%d", rec_url, status, len(text or ""))
                if status == 200 and not _is_challenge_html(text, status):
                    rec_html = text
            except Exception as e:
                logger.debug("cloudscraper rec request failed: %s", e)
        if not rec_html:
            # playwright fallback for rec endpoint
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                rec_html = loop.run_until_complete(_fetch_with_playwright_url(rec_url))
            except Exception as e:
                logger.debug("playwright rec fetch failed: %s", e)
            finally:
                try:
                    loop.close()
                except Exception:
                    pass

        if rec_html:
            recs = _parse_main_and_recommendations(rec_html).get("recommendations", [])
            # merge: add items from recs not already present (by slug)
            seen = {r["slug"] for r in payload.get("recommendations", [])}
            extras = [r for r in recs if r["slug"] not in seen]
            if extras:
                payload["recommendations"].extend(extras)

        # 4) Fetch reviews page (/reviews)
        rv_html = None
        rv_url = f"{ANIMEPLANET_BASE}/manga/{slug}/reviews"
        if session is not None:
            try:
                headers = dict(DEFAULT_HEADERS)
                headers["User-Agent"] = random.choice(USER_AGENTS)
                r = session.get(rv_url, headers=headers, timeout=30)
                status = getattr(r, "status_code", None)
                text = getattr(r, "text", "")
                logger.info("Reviews request %s status=%s len=%d", rv_url, status, len(text or ""))
                if status == 200 and not _is_challenge_html(text, status):
                    rv_html = text
            except Exception as e:
                logger.debug("cloudscraper reviews request failed: %s", e)
        if not rv_html:
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                rv_html = loop.run_until_complete(_fetch_with_playwright_url(rv_url))
            except Exception as e:
                logger.debug("playwright reviews fetch failed: %s", e)
            finally:
                try:
                    loop.close()
                except Exception:
                    pass

        payload["reviews"] = _parse_reviews(rv_html) if rv_html else []
        payload["http"] = {"code": 200}
        payload["status"] = "ok" if (payload.get("reviews") or payload.get("recommendations")) else "no_reviews"
        return payload

    # If we reached here: nothing fetched â€” try scrapy fallback (if available)
    logger.info("All HTTP/Playwright attempts failed for ap_%s â€” trying scrapy spider fallback", slug)
    spider_doc = _run_scrapy_and_read(slug)
    if spider_doc:
        spider_doc.setdefault("source", "animeplanet")
        spider_doc.setdefault("source_id", slug)
        return spider_doc

    # ultimate fallback
    payload.update({"reviews": [], "recommendations": [], "http": {"error": "could_not_fetch"}, "status": "error"})
    return payload


def get_reviews(slug: str) -> List[Dict]:
    """
    Backwards-compatible function: returns reviews list (possibly empty).
    """
    doc = get_full_data(slug)
    return doc.get("reviews", [])
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\extractors\mal.py ---
import logging
from .mal_fetcher import get_full_data

logger = logging.getLogger(__name__)

def collect_mal(mangadex_id: str, source_id: str):
    """
    Collector entrypoint for MyAnimeList.
    - source_id: MAL numeric id
    """
    logger.info(f"Collecting data for MAL ID: {source_id}")
    try:
        payload = get_full_data(source_id)
        payload.setdefault("_id", f"mal_{source_id}")
        payload.setdefault("source", "mal")
        payload.setdefault("source_id", source_id)
        logger.info(f"Collected data for {source_id}: status={payload.get('status')}, "
                    f"Reviews={len(payload.get('reviews', []))}, "
                    f"Recommendations={len(payload.get('recommendations', []))}, "
                    f"Manga Info={bool(payload.get('manga_info', {}))}")
        return payload
    except Exception as e:
        logger.error(f"collect_mal failed for {source_id}: {e}", exc_info=True)
        return {
            "_id": f"mal_{source_id}",
            "source": "mal",
            "source_id": source_id,
            "reviews": [],
            "recommendations": [],
            "manga_info": {},
            "status": "error",
            "http": {"error": str(e)},
        }
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\extractors\mal_fetcher.py ---
import logging
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import requests
import lxml.html
from lxml.cssselect import CSSSelector
from bs4 import BeautifulSoup
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from concurrent.futures import ThreadPoolExecutor
import asyncio
import aiohttp

logger = logging.getLogger(__name__)

MAL_BASE = "https://myanimelist.net"
MAL_API_BASE = "https://api.myanimelist.net/v2"
MAL_CLIENT_ID = ""  # Add your MAL Client ID here if available
MAL_USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0",
]

# Performance config inspired by successful AniList fetcher
TARGET_OBJECTS_PER_HOUR = 87000 / 24  # ~3625 objects/hour
TARGET_REQUESTS_PER_HOUR = TARGET_OBJECTS_PER_HOUR / 3  # ~1208 requests/hour (3 pages per manga)
SECONDS_PER_REQUEST = 3600 / TARGET_REQUESTS_PER_HOUR  # ~3 seconds/request

def _parse_reviews(html: str, mal_id: str) -> List[Dict]:
    """Parse reviews with fallback selectors and improved text parsing"""
    if not html.strip():
        logger.warning(f"Empty reviews HTML for {mal_id}")
        return []
    
    soup = BeautifulSoup(html, "lxml")
    reviews = []
    
    # Updated selectors for reviews
    selectors = ["div.review-element", "div.review-element.js-review-element", "div.borderDark"]
    for selector in selectors:
        review_elements = soup.select(selector)
        if review_elements:
            logger.debug(f"Using selector '{selector}' for {mal_id}: found {len(review_elements)} reviews")
            break
    else:
        logger.warning(f"No reviews found for {mal_id} with selectors: {selectors}")
        return []
    
    for review in review_elements:
        try:
            review_id_elem = review.select_one('div.open a') or review.select_one('a[href*="/reviews/"]')
            review_id = review_id_elem.get('href', '').split('/')[-1] if review_id_elem else ''
            if not review_id:
                logger.debug(f"No review ID found in review element for {mal_id}")
                continue
            
            # Improved text parsing
            review_text_elem = review.select_one('div.text') or review.select_one('div.review-body')
            review_text = ''
            if review_text_elem:
                review_text = ' '.join(review_text_elem.get_text(' ', strip=True).split())
            
            if not review_text or len(review_text) < 5:
                logger.debug(f"Skipping short review for {mal_id}: {review_text[:50]}...")
                continue
            
            reactions_dict = review.get('data-reactions', '')
            reactions = {}
            if reactions_dict:
                try:
                    import json
                    reactions_data = json.loads(reactions_dict)
                    reaction_type_map = ['nice', 'loveIt', 'funny', 'confusing', 'informative', 'wellWritten', 'creative']
                    reactions = {r: c for r, c in zip(reaction_type_map, reactions_data.get('count', ['0']*7))}
                except:
                    logger.debug(f"Error parsing reactions for {mal_id}: {reactions_dict}")
            
            author_elem = review.select_one('div.username a') or review.select_one('div.reviewer a')
            author = author_elem.get_text(strip=True) if author_elem else ''
            
            score_elem = review.select_one('div.rating span.num') or review.select_one('div.score')
            score = score_elem.get_text(strip=True) if score_elem else ''
            
            post_time = review.select_one('div.update_at') or review.select_one('div.date')
            post_time_text = post_time.get_text(strip=True) if post_time else ''
            
            episodes_seen_elem = review.select_one('.tag.preliminary span') or review.select_one('div.episodes-seen')
            episodes_seen = episodes_seen_elem.get_text(strip=True) if episodes_seen_elem else ''
            
            recommendation_elem = review.select_one('.tag.recommended') or review.select_one('.tag.recommendation')
            recommendation_status = recommendation_elem.get_text(strip=True) if recommendation_elem else ''
            
            profile_url_elem = review.select_one('div.thumb a') or review.select_one('div.reviewer a')
            profile_url = profile_url_elem.get('href') if profile_url_elem else ''
            
            profile_img_elem = review.select_one('div.thumb a img') or review.select_one('div.reviewer img')
            profile_img = profile_img_elem.get('src') if profile_img_elem else ''
            
            review_data = {
                'reviewId': review_id,
                'text': review_text[:3000],
                'author': author,
                'score': score,
                'postTime': post_time_text,
                'episodesSeen': episodes_seen,
                'recommendationStatus': recommendation_status,
                'profileUrl': profile_url,
                'profileImage': profile_img,
                **reactions
            }
            
            reviews.append(review_data)
            
        except Exception as e:
            logger.debug(f"Error parsing review for {mal_id}: {e}")
            continue
    
    logger.info(f"Parsed {len(reviews)} reviews for {mal_id}")
    return reviews

def _parse_recommendations(html: str) -> List[Dict]:
    """Parse recommendations with better selectors"""
    if not html.strip():
        logger.warning("Empty recommendations HTML")
        return []
    
    soup = BeautifulSoup(html, "lxml")
    recs = []
    
    selectors = [
        "div.borderClass a[href*='/manga/']",
        "table.anime_detail_related_anime a[href*='/manga/']",
        "div.spaceit_pad a[href*='/manga/']",
        "td a[href*='/manga/']",
        "a[href*='/manga/']:not([href*='/reviews']):not([href*='/userrecs'])"
    ]
    
    for selector in selectors:
        links = soup.select(selector)
        for a in links:
            href = a.get("href", "")
            title = a.get_text(strip=True)
            if "/manga/" not in href or not title or len(title) < 2:
                continue
            try:
                mid = href.split("/manga/")[1].split("/")[0]
                if mid.isdigit():
                    reason = ""
                    parent = a.find_parent('td') or a.find_parent('div')
                    if parent:
                        reason_elem = parent.find_next_sibling()
                        if reason_elem:
                            reason = reason_elem.get_text(strip=True)[:200]
                    
                    recs.append({
                        "id": mid,
                        "title": title,
                        "url": href,
                        "reason": reason
                    })
            except Exception:
                continue
    
    seen = set()
    unique_recs = []
    for rec in recs:
        if rec["id"] not in seen:
            seen.add(rec["id"])
            unique_recs.append(rec)
    
    logger.info(f"Parsed {len(unique_recs)} recommendations")
    return unique_recs[:20]

@retry(
    retry=retry_if_exception_type((requests.exceptions.HTTPError, requests.exceptions.RequestException)),
    wait=wait_exponential(multiplier=1.2, min=3, max=30),  # Faster recovery like AniList
    stop=stop_after_attempt(3),  # Reduced attempts for speed
    reraise=True
)
def _fetch_page(url: str, session: requests.Session, headers: Dict, mal_id: str, page_type: str, use_selenium: bool = False) -> str:
    """Helper function to fetch a single page with retries using session or Selenium"""
    # Smart delay based on AniList success pattern
    delay = random.uniform(1, 3)  # Reduced from 2-4s to 1-3s
    time.sleep(delay)
    
    if use_selenium:
        options = Options()
        options.add_argument("--headless")
        options.add_argument(f"user-agent={random.choice(MAL_USER_AGENTS)}")
        driver = webdriver.Chrome(options=options)
        try:
            driver.get(url)
            time.sleep(2)  # Wait for JavaScript to load
            html = driver.page_source
            logger.debug(f"Selenium {page_type} response for {mal_id}: length={len(html)}")
            
            # Save HTML for debugging
            temp_folder = Path('tmp/mal_data')
            temp_folder.mkdir(parents=True, exist_ok=True)
            with open(temp_folder / f'mal_{mal_id}_{page_type}_selenium.html', 'w', encoding='utf-8') as f:
                f.write(html)
            return html
        finally:
            driver.quit()
    else:
        response = session.get(url, headers=headers, timeout=20)
        logger.debug(f"{page_type} response for {mal_id}: {response.status_code}, length={len(response.text)}")
        
        # Save HTML for debugging
        temp_folder = Path('tmp/mal_data')
        temp_folder.mkdir(parents=True, exist_ok=True)
        with open(temp_folder / f'mal_{mal_id}_{page_type}.html', 'w', encoding='utf-8') as f:
            f.write(response.text if response.ok else '')
        
        if response.status_code == 405:
            logger.warning(f"HTTP 405 for {mal_id} {page_type} - MAL ID may not exist or be invalid")
            return ""
        response.raise_for_status()
        return response.text

def _fetch_mal_comprehensive(mal_id: str, use_selenium: bool = False) -> tuple[str, str, List[Dict]]:
    """Comprehensive scraping: main page + recommendations + all review pages"""
    headers = {
        "User-Agent": random.choice(MAL_USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Referer": "https://myanimelist.net/",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "same-origin",
    }
    
    session = requests.Session()
    start_time = time.time()
    
    # Fetch main page
    main_html = _fetch_page(f"{MAL_BASE}/manga/{mal_id}", session, headers, mal_id, "main", use_selenium)
    
    # Fetch recommendations
    recs_html = _fetch_page(f"{MAL_BASE}/manga/{mal_id}/_/userrecs", session, headers, mal_id, "recs", use_selenium)
    
    # Fetch reviews with pagination
    reviews = []
    page = 1
    while True:
        # Simplified URL to avoid filtering
        review_url = f"{MAL_BASE}/manga/{mal_id}/reviews?p={page}"
        try:
            review_html = _fetch_page(review_url, session, headers, mal_id, f"reviews_page_{page}", use_selenium)
            page_reviews = _parse_reviews(review_html, mal_id)
            reviews.extend(page_reviews)
            
            if not page_reviews:
                logger.info(f"No reviews found for {mal_id} on page {page}")
                break
            
            # Check for next page
            soup = BeautifulSoup(review_html, "lxml")
            next_page_selectors = [
                'a.ga-click[data-ga-click-type="review-more-reviews"]',
                'a.button[rel="next"]',
                'div.pagination a.next',
                'a[href*="reviews?p="]:not([href*="p=1"])',  # Avoid page 1
                'a.js-reviews-pagination',
                'div.mt4 a[href*="reviews?p="]',  # Additional selector from inspection
                'a[href*="reviews?p=%s"]' % (page + 1),  # Specific next page
            ]
            next_page_url = None
            for selector in next_page_selectors:
                next_page = soup.select_one(selector)
                if next_page and next_page.get('href'):
                    next_page_url = next_page.get('href')
                    if not next_page_url.startswith('http'):
                        next_page_url = MAL_BASE + next_page_url
                    logger.debug(f"Found next page for {mal_id} using selector '{selector}': {next_page_url}")
                    break
            if not next_page_url:
                logger.info(f"No next page for reviews of {mal_id} after page {page}")
                break
            page += 1
        except Exception as e:
            logger.warning(f"Error fetching reviews page {page} for {mal_id}: {e}")
            break
    
    session.close()
    logger.info(f"Completed fetching {mal_id} in {time.time() - start_time:.2f} seconds: {len(reviews)} reviews, {page-1} pages")
    return main_html, recs_html, reviews

def _parse_manga_info(html: str, mal_id: str) -> Dict:
    """Parse comprehensive manga info from main page"""
    if not html.strip():
        logger.warning(f"Empty main page HTML for {mal_id}")
        return {}
    
    tree = lxml.html.fromstring(html)
    info = {}
    
    try:
        info['jpName'] = tree.xpath('//span[contains(text(), "Japanese:")]/following::text()')[0].strip() if tree.xpath('//span[contains(text(), "Japanese:")]/following::text()') else ''
        info['engName'] = tree.xpath('//span[contains(text(), "English:")]/following::text()')[0].strip() if tree.xpath('//span[contains(text(), "English:")]/following::text()') else ''
        info['synonyms'] = tree.xpath('//span[contains(text(), "Synonyms:")]/following::text()')[0].strip() if tree.xpath('//span[contains(text(), "Synonyms:")]/following::text()') else ''
        info['type'] = tree.xpath('//span[text()="Type:"]/following-sibling::a/text()')[0] if tree.xpath('//span[text()="Type:"]/following-sibling::a/text()') else ''
        info['volumes'] = tree.xpath('//span[text()="Volumes:"]/following::text()')[0].strip() if tree.xpath('//span[text()="Volumes:"]/following::text()') else ''
        info['chapters'] = tree.xpath('//span[text()="Chapters:"]/following::text()')[0].strip() if tree.xpath('//span[text()="Chapters:"]/following::text()') else ''
        info['status'] = tree.xpath('//span[text()="Status:"]/following::text()')[0].strip() if tree.xpath('//span[text()="Status:"]/following::text()') else ''
        info['published'] = tree.xpath('//span[text()="Published:"]/following::text()')[0].strip() if tree.xpath('//span[text()="Published:"]/following::text()') else ''
        info['genres'] = ', '.join(str(genre) for genre in tree.xpath('//span[text()="Genres:"]/following-sibling::a/text()')) if tree.xpath('//span[text()="Genres:"]/following-sibling::a/text()') else ''
        info['themes'] = ', '.join(str(theme) for theme in tree.xpath('//span[text()="Themes:"]/following-sibling::a/text()')) if tree.xpath('//span[text()="Themes:"]/following-sibling::a/text()') else ''
        info['demographic'] = tree.xpath('//span[text()="Demographic:"]/following-sibling::a/text()')[0] if tree.xpath('//span[text()="Demographic:"]/following-sibling::a/text()') else ''
        info['serialization'] = ', '.join(str(s) for s in tree.xpath('//span[text()="Serialization:"]/following-sibling::a/text()')) if tree.xpath('//span[text()="Serialization:"]/following-sibling::a/text()') else ''
        info['authors'] = ', '.join(str(author) for author in tree.xpath('//span[text()="Authors:"]/following-sibling::a/text()')) if tree.xpath('//span[text()="Authors:"]/following-sibling::a/text()') else ''
        info['score'] = CSSSelector('span.score-label')(tree)[0].text if CSSSelector('span.score-label')(tree) else ''
        info['ranked'] = tree.xpath('//span[text()="Ranked:"]/following::text()')[0].strip() if tree.xpath('//span[text()="Ranked:"]/following::text()') else ''
        info['popularity'] = tree.xpath('//span[text()="Popularity:"]/following::text()')[0].strip() if tree.xpath('//span[text()="Popularity:"]/following::text()') else ''
        info['members'] = tree.xpath('//span[text()="Members:"]/following::text()')[0].strip() if tree.xpath('//span[text()="Members:"]/following::text()') else ''
        info['favorites'] = tree.xpath('//span[text()="Favorites:"]/following::text()')[0].strip() if tree.xpath('//span[text()="Favorites:"]/following::text()') else ''
        info['cover_image'] = CSSSelector('div.leftside img.lazyload')(tree)[0].get('src') or CSSSelector('div.leftside img.lazyload')(tree)[0].get('data-src') if CSSSelector('div.leftside img.lazyload')(tree) else ''
        info['synopsis'] = tree.xpath('//span[@itemprop="description"]/text()')[0].strip() if tree.xpath('//span[@itemprop="description"]/text()') else ''
        
        logger.info(f"Parsed manga info for {mal_id}: {bool(info)}")
    except Exception as e:
        logger.warning(f"Error parsing manga info for {mal_id}: {e}")
    
    return info

def get_full_data(mal_id: str, use_selenium: bool = False) -> Dict:
    payload = {
        "_id": f"mal_{mal_id}",
        "source": "mal",
        "source_id": mal_id,
        "source_url": f"{MAL_BASE}/manga/{mal_id}",
        "fetched_at": datetime.utcnow().isoformat(),
    }
    
    try:
        # Comprehensive web scraping approach
        main_html, recs_html, reviews = _fetch_mal_comprehensive(mal_id, use_selenium)
        
        # Parse all data
        payload["manga_info"] = _parse_manga_info(main_html, mal_id)
        payload["recommendations"] = _parse_recommendations(recs_html) if recs_html else []
        payload["reviews"] = reviews if reviews else []
        
        # Determine status
        has_data = bool(payload["reviews"] or payload["recommendations"] or payload["manga_info"])
        payload["status"] = "ok" if has_data else "no_reviews"
        payload["http"] = {"code": 200} if has_data else {"error": "no_data"}
        
        logger.info(f"MAL comprehensive for {mal_id}: {len(payload['reviews'])} reviews, {len(payload['recommendations'])} recs, info: {bool(payload['manga_info'])}")
        
    except Exception as e:
        logger.error(f"MAL fetch failed for {mal_id}: {e}", exc_info=True)
        payload.update({"recommendations": [], "reviews": [], "manga_info": {}, "status": "error", "http": {"error": str(e)}})

    return payload
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\extractors\mangaupdates.py ---
import logging
from .mangaupdates_fetcher import get_full_data

logger = logging.getLogger(__name__)

def collect_mangaupdates(mangadex_id: str, source_id: str):
    """
    Collector entrypoint for MangaUpdates.
    - source_id: MU numeric id or slug
    """
    try:
        payload = get_full_data(source_id)
        payload.setdefault("_id", f"mu_{source_id}")
        payload.setdefault("source", "mangaupdates")
        payload.setdefault("source_id", source_id)
        return payload
    except Exception as e:
        logger.error("collect_mangaupdates failed for %s: %s", source_id, e, exc_info=True)
        return {
            "_id": f"mu_{source_id}",
            "source": "mangaupdates",
            "source_id": source_id,
            "reviews": [],
            "recommendations": [],
            "status": "error",
            "http": {"error": str(e)},
        }
--- File: D:\Projects\Há»c DE\data-engineering-learning\2025-08-18-fetch-mangadata-from-famous-websites\src\extractors\mangaupdates_fetcher.py ---
import logging
from datetime import datetime
from typing import Dict, List
import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

MU_BASE = "https://www.mangaupdates.com"


def _parse_reviews(html: str) -> List[Dict]:
    soup = BeautifulSoup(html, "lxml")
    reviews = []
    for div in soup.select(".sMemberComment, .commentText"):
        text = div.get_text(" ", strip=True)
        if text:
            reviews.append({"text": text})
    return reviews


def _parse_recommendations(html: str) -> List[Dict]:
    soup = BeautifulSoup(html, "lxml")
    recs = []
    for a in soup.select("a[href*='/series/']"):
        href = a.get("href", "")
        if "/series/" not in href:
            continue
        sid = href.split("/series/")[-1].split("?")[0]
        recs.append({"id": sid, "url": href})
    return recs


def get_full_data(mu_id: str) -> Dict:
    payload = {
        "_id": f"mu_{mu_id}",
        "source": "mangaupdates",
        "source_id": mu_id,
        "source_url": f"{MU_BASE}/series/{mu_id}",
        "fetched_at": datetime.utcnow().isoformat(),
    }
    try:
        r1 = requests.get(f"{MU_BASE}/series.html?id={mu_id}", timeout=20)
        payload["recommendations"] = _parse_recommendations(r1.text) if r1.ok else []
        payload["reviews"] = _parse_reviews(r1.text) if r1.ok else []
        payload["status"] = "ok" if (payload["reviews"] or payload["recommendations"]) else "no_reviews"
        payload["http"] = {"code": 200}
    except Exception as e:
        logger.error("MangaUpdates fetch failed: %s", e, exc_info=True)
        payload.update({"recommendations": [], "reviews": [], "status": "error", "http": {"error": str(e)}})

    return payload
=== Folder: tmp ===

=== Folder: tmp\mal_data ===
